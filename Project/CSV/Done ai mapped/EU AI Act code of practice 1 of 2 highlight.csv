text_extract;highlight_color
"Sub-Measures and KPIs should be specific. We accept that Measures may be written at a higher level of generality than Sub-Measures and the KPIs evidencing compliance with Sub-Measures. Nonetheless, general-purpose AI model providers should have as clear of an understanding as possible of how to satisfy Sub-Measures, as appropriate, evidenced by KPIs.";green
"Measures, Sub-Measures and KPIs should be more stringent for more significant risks or uncertain risks of severe harm. The Code can accomplish this by, for example, suggesting a multitude of KPIs for each Sub-Measure related to a severe risk, thereby requiring providers of general-purpose AI models to take action to mitigate that severe risk or to robustly demonstrate an extremely rare likelihood of severe risk eventuating.";green
"Measures, Sub-Measures and KPIs should differentiate, where applicable, between different types of risks, distribution strategies, deployment contexts, and other factors that may influence the level of risk, and how risks may need to be assessed and mitigated.";green
"The Code might also tie risk-mitigating Sub-Measures to risk-assessment KPIs, such as using "if-then" requirements. For example, if a general-purpose AI model with systemic risk is assessed to have capability X, Y risk mitigations must be in place, guided by Z KPIs.";green
"The Code can accomplish this by, for example, referencing dynamic sources of information that providers can be expected to monitor and consider themselves. Examples of such sources could include incident databases, consensus standards, risk registers, risk management frameworks, and AI Office guidance.";yellow
"Measures and KPIs related to the obligations applicable to providers of general-purpose AI models should take due account of the size of the general-purpose AI model provider and allow simplified ways of compliance for SMEs and start-ups with fewer financial resources than those at the frontier of AI development, where appropriate.";green
"We encourage further transparency between stakeholders and increased efforts to share knowledge and cooperate in building a collective and robust evidence base for AI Safety.";yellow
"Measures, Sub-Measures and KPIs should differentiate between intentional and unintentional (including misalignment) risks, and might be more or less specific and stringent for some types of risks, distribution strategies (e.g. open-sourcing), and deployment contexts than for others.";green
"Sub-Measures and KPIs should preserve the AI Office's ability to improve its assessment of compliance based on superior information.";yellow
"The process for updating Sub-Measures and KPIs should assume that rapid technological change may require agile regulatory development and modification.";yellow
"The Code serves as a guiding document for providers of GPAI models and general-purpose AI models with systemic risk in demonstrating compliance with the AI Act, while recognising that adherence to this Code does not constitute conclusive evidence of compliance with the AI Act.";yellow
"The absence of specific Measures, Sub-Measures, and Key Performance Indicators (KPIs) within this Code does not absolve providers of general-purpose AI models with systemic risk of their responsibility to address and mitigate potential systemic risks as they emerge.";green
"The Signatories recognise the particular role and responsibility of providers of general-purpose AI models along the AI value chain, as the models they provide may form the basis for a range of downstream systems.";red
"Detailed documentation requirements for AI Office and downstream providers including: model name, evidence of provenance and authenticity, intended tasks and types of AI systems for integration, acceptable use policies, date of release and distribution methods, interaction with external hardware/software, architecture details, input/output modalities, licensing information, technical integration means, design specifications, training process details, data usage information, computational resources used, and energy consumption metrics.";green
"The AUP should contain: purpose statement, scope definition, primary intended uses and users, acceptable/unacceptable uses, security measures, monitoring and privacy policies, warning processes, suspension criteria, and user acknowledgment requirements.";green
"The AUP should contain details about the security protocols that the users of the general-purpose AI systems must follow.";green
"Warning processes and criteria for suspension or withdrawal of user privileges for not adhering to the AUP.";green
"Criteria for terminating user accounts and reference to applicable law and regulations for enforcement.";green
"Signatories will draw up and implement an internal policy to comply with Union law on copyright and related rights in line with this Chapter of the Code. This policy shall cover the entire lifecycle of any general-purpose AI model subject to this Code.";green
"Signatories will undertake reasonable copyright due diligence before entering into contracts with third parties about data usage. Implementation of downstream copyright measures to mitigate risks of copyright infringement.";green
"When engaging in text and data mining, signatories must ensure lawful access to copyright-protected content and comply with rights reservations. This includes respecting robots.txt protocols and avoiding negative impacts on content findability.";green
"Signatories commit to adequate transparency about copyright compliance measures, including making public information about rights reservation compliance and providing a single point of contact for rightsholders.";green
"Issues related to the inability to control powerful autonomous general-purpose AI models.";red
"The Signatories recognise that the taxonomy of systemic risks is non-exhaustive and will be subject to change over time, reflecting scientific advances and societal changes.";yellow
"The taxonomy has been developed and, when in doubt, should be interpreted in light of the severity and probability of each risk as defined in Article 3(2) of the AI Act.";green
"Taxonomy of systemic risks including: cyber offence, CBRN risks, loss of control, automated AI R&D risks, manipulation risks, and large-scale discrimination. Consideration of accidents, privacy infringements, and impacts on public health, safety, democracy, infrastructure, rights, environment, and economy.";yellow
"Sources of risks, also referred to as "factors of risks" or "drivers of risks," are elements (e.g. events, components, actors and their intentions or activities) that alone or in combination give rise to risks.";red
"Velocity at which the risk materialises: Gradual, sudden, continuously changing.";red
"Visibility of the risk while it materialises: Overt (open), covert (hidden).";red
"Course of events: Linear, recursive (feedback loops), compound, cascading (chain reactions).";red