text_extract;highlight_color
"Autonomy, scalability, adaptability to learn new tasks";red
"Dangerous model propensities";red
"Model characteristics beyond capabilities that may cause systemic risk";yellow
"Misalignment with human intent and/or values";red
"Tendency to deceive";red
"Bias";red
"Confabulation";red
"Lack of reliability and security";red
"'Goal-pursuing', resistance to goal modification, and 'power-seeking'";red
"'Colluding' with other AI models/systems to do so";red
"Potential to remove guardrails";yellow
"Access to tools (including other models)";yellow
"Modalities (including novel and combined modalities)";red
"Human oversight";green
"Model exfiltration (e.g. model leakage/theft)";yellow
"Number of business users and number of end-users";red
"Offence-defence balance, including the number, capacity, and willingness of bad actors to misuse the model";yellow
"Societal vulnerability or adaptation";red
"Lack of explainability or transparency";yellow
"Technology readiness (i.e. how mature a technology is within a given application context)";yellow
"Feedback loops in the use of data, model, and inferences";yellow
"perform model evaluation in accordance with standardised protocols and tools reflecting the state of the art, including conducting and documenting adversarial testing of the model with a view to identifying and mitigating systemic risks";green
"assess and mitigate possible systemic risks at Union level, including their sources, that may stem from the development, the placing on the market, or the use of general-purpose AI models with systemic risk";green
"keep track of, document, and report, without undue delay, to the AI Office and, as appropriate, to national competent authorities, relevant information about serious incidents and possible corrective measures to address them";green
"ensure an adequate level of cybersecurity protection for the general-purpose AI model with systemic risk and the physical infrastructure of the model";green
"a tier of severity at which the level of risk would be considered intolerable absent appropriate safeguards";green
"providers of general-purpose AI models with systemic risk should continuously assess and mitigate systemic risks, taking appropriate measures along the entire model's lifecycle, cooperating with relevant actors along the AI value chain, and ensuring their risk management is future-proof by regularly updating their practices in light of improving and emerging capabilities";green
"detailed risk assessment, mitigations, and documentation are particularly important where the general-purpose AI model with systemic risk is more likely to (i) present substantial systemic risk, (ii) has uncertain capabilities and impacts, or (iii) where the provider lacks relevant expertise";green
"there is less need for more comprehensive measures where there is good reason to believe that a new general-purpose AI model will exhibit the same high-impact capabilities as exhibited by general-purpose AI models with systemic risk that have already been safely deployed, without significant systemic risks materialising and where the implementation of appropriate mitigations has been sufficient";green
"proportionality to size and capacity of providers";yellow
"to account for differences in available resources between providers of different size and capacity, and recognising the principle of proportionality, simplified ways of compliance for SMEs and startups will be provided where appropriate";green
"there are a wide range of organisations that have significant expertise and are well placed to assist with the assessment and mitigation of systemic risks";yellow
"they encourage each other to 'share the load', for example by sharing evaluations, best practices or infrastructure, or – where appropriate – by working with qualified third-party providers, potentially facilitated by industry organisations";green
"The Signatories commit to adopting, implementing, and making available a Safety and Security Framework (SSF), which shall detail the risk management policies they adhere to in order to proactively assess and proportionately mitigate systemic risks from their general-purpose AI models with systemic risks";green
"As part of their SSF, Signatories commit to continuously and thoroughly identifying systemic risks that may stem from the general-purpose AI model with systemic risk";green
"Signatories will determine and specify the systemic risks that are particularly relevant to the proposed development, placing on the market, or use of the general-purpose AI model with systemic risk";green
"Signatories will use robust risk analysis methodologies to identify the pathways by which the development and deployment of their general-purpose AI model with systemic risk could produce the systemic risks identified, as well as the probability of such risks materialising through those pathways";green
"For their general-purpose AI models with systemic risk, Signatories will identify and map potentially dangerous model capabilities, propensities, and other sources of risk that may enable the pathways to systemic risks identified, and provide systemic risk indicators for each of these elements";green
"For their general-purpose AI models with systemic risk, Signatories will categorise the identified dangerous model capabilities, dangerous model propensities, and other sources of risk into tiers of severity";green
"Signatories will include in their SSF best effort estimates of timelines for when they expect to develop a model that triggers the systemic risk indicators";green
"Where applicable to their general-purpose AI models with systemic risk, Signatories will collect model-agnostic evidence of the systemic risks presented by their model, using a wide range of methods that may include literature reviews, competitor and open-source project analysis, forecasting of general trends, and participatory methods involving civil society, academia, and other relevant stakeholders";green
"Signatories will ensure best-in-class evaluations are run to adequately assess the capabilities and limitations of their general-purpose AI models with systemic risks";green
"like all evidence collection in this section, this may be done in collaboration with – or outsourced to – qualified third parties";green
"Signatories will ensure the execution of evaluations with high scientific rigour. Additional rigour shall be achieved through the validation of key results by qualified third parties, especially for high tiers of severity of systemic risks";green
"Signatories will ensure that evaluations are being run with a best-in-class level of capability elicitation (e.g. fine-tuning, prompt engineering, scaffolding, compute and engineering budgets) to fully elicit the capabilities of a model and minimise the risk of under-estimating capabilities";green
"Signatories will ensure that evaluations can assess the capabilities and limitations of a general-purpose AI model with systemic risk, both in an AI system representative of future AI systems in which the model is intended to and reasonably foreseeably will be used";green
"Signatories will ensure that evaluations match the planned usage context of a model with all its variety, where applicable, to show generalisation";green
"Signatories will ensure that significant amounts of exploratory work are done on their general-purpose models with systemic risk, such as open-ended red teaming by qualified third parties";green
"This means that they will not restrict themselves only to evidence collection for risks or capabilities they have already identified, but also strive to identify new risks and emerging capabilities through these methods";green
"Signatories will strive to make best-in-class safety evaluations, tooling, and accompanying best practices widely accessible to relevant actors in the AI ecosystem";green
"When Signatories share evaluation results with the AI Office or the public, they will do so in a transparent and easily comparable format. They shall transparently report uncertainty of any empirical results and limitations of the methods used";green
"Signatories commit to continuously assess risks and collect evidence during the full lifecycle of the development and deployment of general-purpose AI models with systemic risk";green
"In specifically identified cases, Signatories may limit the sharing of information to protect commercially sensitive information, public security, proliferation risks, and the validity of future evaluations";green
"Before starting a training run for a general-purpose AI model with systemic risk, Signatories will make updates to the SSF as necessary and ensure evaluators (internal and external) are ready for Evidence Collection";green
"Signatories will collect evidence at regular milestones, updating an in-progress Safety and Security Report (SSR) as commensurate with the risks";green
"During the deployment of any general-purpose AI model with systemic risk, Signatories will update the model's SSR by revisiting their risk assessment, especially by re-running relevant evaluations at least every six months";green
"Signatories will conduct post-deployment monitoring for systemic risks. They will establish mechanisms to continuously gather and include relevant post-deployment information in risk assessment";green
"Signatories will detail in their SSF the security mitigations they will implement to mitigate systemic risk from the possession of (a) the unreleased weights of a general-purpose AI model with systemic risk, and (b) related unreleased assets and information necessary to train or use such unreleased models";green
"These security mitigations should furthermore be proportional to systemic risk indicators or tiers of severity, and could entail (a) protection of weights and assets at-rest, in-motion, and in-use, including at the hardware-level as appropriate (b) access control, monitoring, and hardened interfaces to weights and assets, (c) assurance through ongoing security red-teaming and accredited security reviews, and (d) screening for insider threats";green
"protect commercially sensitive information, public security, proliferation risks, and the validity of future evaluations";green
"Signatories will detail in their SSF their process for assessing the continued adequacy of their mapping from systemic risk indictors or tiers of severity to safety and security mitigations";green
"Signatories will ensure an SSR has sufficient scientific detail to allow for the independent assessment of the methods used to generate the results, evidence, and analysis";green
"while protecting intellectual property rights and confidential business information where appropriate";greentext_extract;highlight_color
"Autonomy, scalability, adaptability to learn new tasks";red
"Dangerous model propensities";red
"Model characteristics beyond capabilities that may cause systemic risk";yellow
"Misalignment with human intent and/or values";red
"Tendency to deceive";red
"Bias";red
"Confabulation";red
"Lack of reliability and security";red
"'Goal-pursuing', resistance to goal modification, and 'power-seeking'";red
"'Colluding' with other AI models/systems to do so";red
"Potential to remove guardrails";yellow
"Access to tools (including other models)";yellow
"Modalities (including novel and combined modalities)";red
"Human oversight";green
"Model exfiltration (e.g. model leakage/theft)";yellow
"Number of business users and number of end-users";red
"Offence-defence balance, including the number, capacity, and willingness of bad actors to misuse the model";yellow
"Societal vulnerability or adaptation";red
"Lack of explainability or transparency";yellow
"Technology readiness (i.e. how mature a technology is within a given application context)";yellow
"Feedback loops in the use of data, model, and inferences";yellow
"perform model evaluation in accordance with standardised protocols and tools reflecting the state of the art, including conducting and documenting adversarial testing of the model with a view to identifying and mitigating systemic risks";green
"assess and mitigate possible systemic risks at Union level, including their sources, that may stem from the development, the placing on the market, or the use of general-purpose AI models with systemic risk";green
"keep track of, document, and report, without undue delay, to the AI Office and, as appropriate, to national competent authorities, relevant information about serious incidents and possible corrective measures to address them";green
"ensure an adequate level of cybersecurity protection for the general-purpose AI model with systemic risk and the physical infrastructure of the model";green
"a tier of severity at which the level of risk would be considered intolerable absent appropriate safeguards";green
"providers of general-purpose AI models with systemic risk should continuously assess and mitigate systemic risks, taking appropriate measures along the entire model's lifecycle, cooperating with relevant actors along the AI value chain, and ensuring their risk management is future-proof by regularly updating their practices in light of improving and emerging capabilities";green
"detailed risk assessment, mitigations, and documentation are particularly important where the general-purpose AI model with systemic risk is more likely to (i) present substantial systemic risk, (ii) has uncertain capabilities and impacts, or (iii) where the provider lacks relevant expertise";green
"there is less need for more comprehensive measures where there is good reason to believe that a new general-purpose AI model will exhibit the same high-impact capabilities as exhibited by general-purpose AI models with systemic risk that have already been safely deployed, without significant systemic risks materialising and where the implementation of appropriate mitigations has been sufficient";green
"proportionality to size and capacity of providers";yellow
"to account for differences in available resources between providers of different size and capacity, and recognising the principle of proportionality, simplified ways of compliance for SMEs and startups will be provided where appropriate";green
"there are a wide range of organisations that have significant expertise and are well placed to assist with the assessment and mitigation of systemic risks";yellow
"they encourage each other to 'share the load', for example by sharing evaluations, best practices or infrastructure, or – where appropriate – by working with qualified third-party providers, potentially facilitated by industry organisations";green
"The Signatories commit to adopting, implementing, and making available a Safety and Security Framework (SSF), which shall detail the risk management policies they adhere to in order to proactively assess and proportionately mitigate systemic risks from their general-purpose AI models with systemic risks";green