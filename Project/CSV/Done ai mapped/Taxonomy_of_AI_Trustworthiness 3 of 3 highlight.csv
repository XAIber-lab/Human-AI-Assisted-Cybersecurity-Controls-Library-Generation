text_extract;highlight_color
"How will we assess and improve the completeness, quantity, suitability, and representativeness of the data?";green
"How will we assess and improve the quality and relevance of the data? What benchmarks will we use? How will we collect and process data, for example to annotate, label, clean, and aggregate as needed?";green
"How will we obtain data, and what are our informational flows? How will we appropriately limit the scope of our data collection? How will we retain and delete data as needed?";green
"How will we analyze and monitor for data drift over time?";green
"How will we assess and improve the balance and diversity of the data? How will we evaluate all data sets for inclusion and representation of demographic groups? How will we guard against proxies for demographic information that could contribute to discrimination?";green
"How will the security of data that is used for training or created be ensured?";green
"How will we protect the data used to build and operate the AI system? How will we use encryption, differential privacy, federated learning, data minimization, and/or other best practices to protect data?";green
"How will we establish data oversight mechanisms, such as limiting and logging data access?";green
"How will we enable people to consent to the uses of their data?";green
"How will we ensure people have a say in how information about them is used? How will we honor the right to rectification and the right to erasure?";green
"How will we analyze and follow data governance practices for all intended uses, stakeholders, and relevant geographic areas? How will we ensure data rights and agency?";green
"How will we document the provenance of data, processes, and artifacts involved in the production of the AI system?";green
"How will we assess the accuracy of what the model has learned using an interpretation method (descriptive accuracy)? How will we assess the accuracy of the underlying data relationships with the model (predictive accuracy)? What benchmarks will we use? How will we communicate this as needed?";green
"How will we test whether desirable outputs of the AI system can be reproduced in different circumstances?";green
"How will we improve the efficiency of the AI system in terms of its energy and power usage, model size, and memory consumption? How can we make the model architecture of the AI system more efficient?";green
"How will we ensure that reliable technical and procedural controls, including deactivation and fail-safe shutdown, are in place to enable the safe use of the AI system?";green
"How will we test the ability of the AI system to try to 'game' a proxy of a true objective function, or to learn novel methods to achieve its objective function? How will this be prevented?";green
"How will we review any errors or inconsistencies with the AI system that emerge?";green
"To whom or what will the AI system be 'loyal,' and will that be optimal and made transparent?";yellow
"How will we incentivize models to avoid power or avoid gaining more power than is necessary?";yellow
"How can we contain the AI system to prevent safety and security breaches?";green
"How will we assess and mitigate computational bias (including biased input data and biased model design)? How will we ensure the AI system does not provide a lower quality of service for certain demographic groups, including marginalized groups?";green
"How will we detect if there is hidden functionality embedded in our models?";green
"How will the AI system respond to attacks as they occur?";green
"How will we make model uncertainty more interpretable by adding features such as confidence interval outputs, conditional probabilistic predictions encoded through sentences, and calibration?";green
"How will we protect model access that could reveal sensitive information?";green
"How will we ensure the AI system only presents outputs that are accurate and not intentionally deceptive?";green
"How can we reduce the computational requirements of the AI system?";yellow
"How will we ensure that the AI system can generalize from the testing environment to the complexity or different context of the application environment?";green
"How will we assess the complexity of integrated networks and dependencies required for the functioning of the AI system?";green
"How will we test the usability of the AI system for all kinds of users and facilitate user feedback? How will the user interface be tested for usability, comprehension, and other attributes? How will we ensure users know how to interpret system behavior?";green
"How will we ensure that the AI system's user interface is usable by those with special needs or disabilities, or those at risk of exclusion?";green
"How will we judge the interpretability of the system's explanation to the particular context and user?";green
"How will we assess potential risks of publicizing, publishing, opening up for external use, or open-sourcing an AI system's code or model? How will we determine a strategy to safely and appropriately release the AI system, and what protections may be necessary to prevent harm or misuse?";green
"How will we share critical information about our AI system with relevant authorities and stakeholders?";green
"How will we test the system with users, and how will we engage them in iterating upon the system design and deployment? How will we test and improve the user experience?";green
"How can we inform users that they are interacting with an AI system (and what type of AI system), or that a decision that impacts them was made by an AI system, and how can we provide expectations as to the system's capabilities, benefits, and limitations and potential risks?";green
"How will we ensure the AI system will be leveraged to benefit society?";yellow
"How will we monitor the AI system's capabilities, outputs, errors, breaches, success, and impacts over time, especially for self-learning or continuous-learning AI systems? How will we determine which events to monitor, and how to prioritize review and response?";green
"How will we ensure the maintainability of the AI system after it is operationalized? How will we maintain the quality of the system and its outputs over time?";green
"How will we ensure that a human is in control or meaningfully in the loop of the operational decision-making process of the AI system, and has been trained to exercise oversight and avoid overconfidence in the system?";green
"How will human oversight be ensured in the operation of the AI system? How will we designate and train the stakeholders responsible for managing and monitoring the AI system, including overriding or interrupting the system if necessary?";green
"How will we determine when and how to retire the use of the AI system?";green
"How will we continue to learn, iterate, and improve over time?";yellow
"How will we evaluate when the AI system has been sufficiently modified such that a new review of its technical robustness and safety is warranted?";green
"How will we assess shifts to an AI system if it learns and evolves over time, including the possibility of emerging properties or discontinuous jumps in capabilities?";green
"How will we track shifts in the AI system's functionality over time?";green
"How will we predict and detect new capabilities and goals of the AI system?";green
"How will we identify and prevent or mitigate and minimize significant adverse impacts, including harm and/or violence to people or communities, including harassment, stereotyping or demeaning, addiction, or over-reliance?";green
"How will we monitor and prevent or mitigate the creation or spread of malicious or harmful synthetic content, such as non-consensual deepfakes?";green
"How will we monitor uses and actively prevent or mitigate misuses and abuses, including human rights abuses? For example, how will we prevent the sale or the system to actors with records of human rights abuses?";green
"How will we monitor and prevent or mitigate individual or social manipulation, for example through recommender systems, dark patterns, or computational propaganda?";green
"How will we analyze and document the environmental implications of the AI system and its uses?";green
"How will we determine which third parties to do business with, and how will we oversee third-party uses to help prevent misuses of the AI system?";green
"How will we assess the implications of the use of the AI system over time? What events should trigger reevaluation, and how frequently should we reevaluate?";green
"How will we identify and engage with communities impacted by the use of the system, either directly or indirectly, and incorporate their feedback?";green
"How will we establish a dedicated channel for feedback and questions about the AI system from users and the general public?";green
"How will we publicly report incidents and adverse impacts of the AI system, such as mistakes, errors, breaches, unintended consequences, etc.?";green
"How will we establish a coordinated policy to encourage responsible vulnerability research and disclosure?";green
"How will we notify users and impacted communities about privacy or security breaches, or other incidents?";green
"How will users be able to contest or appeal a decision or action made by the AI system?";green
"How will we support or compensate people who are negatively affected by the use of the AI system?";green
"How can we enable access to the AI system and datasets to relevant authorities, independent researchers, and trusted intermediaries?";green
"How will we enable users of the AI system to consent to its use? How will we enable them to withdraw consent?";green
"How will we ensure that people have specific and clear opportunities to opt out of use of the AI system?";green
"How will we protect consumers or users of the system from harm?";green
"How will we protect whistleblowers, NGOs, trade unions, or other entities who come forward with concerns about the AI system?";green
"How will we analyze and follow global governance deliberations and practices related to artificial intelligence?";yellow
"The taxonomy may serve as a resource and tool for organizations developing AI, as well as for standards-setting bodies, policymakers, independent auditors, and civil society organizations working to evaluate and promote trustworthy AI";red
"One of the interesting discoveries of this research is that there are properties of trustworthiness that are unlikely to be relevant to AI systems that have minimal human engagement";red
"However, it is notable that the majority of the properties remain important across the spectrum of human engagement. This is largely because effectively all AI systems are designed and developed by people, and can impact our shared environment or high-stakes settings, even if they are not ultimately used or operated by people";yellow
"It is also still important to consider properties such as justice and the protection of human rights, which include aspects of design and impact beyond immediate engagement with or use of the system";yellow
"It is interesting that all of the properties related to safety and security are likely to be relevant irrespective of the degree of human engagement. We believe this to be the case because people expect technological tools to be safe and secure regardless of use";red
"The impact of safety or security failures will vary depending on how the AI system is used, and higher standards are typical for more high-stakes settings";yellow
"The segmentation of the properties of trustworthiness across parts of the AI lifecycle, including connecting them to available tools and resources for implementation as found in the NIST AI RMF, is intended to provide further nuance and practicality";red
"Further research would be useful to pilot the use of this framework with organizational teams and to develop case studies for using the taxonomy across different domains";red
"Higher standards are necessary when dealing with sensitive data or operating in high-stakes environments where failures could have severe consequences";yellow
