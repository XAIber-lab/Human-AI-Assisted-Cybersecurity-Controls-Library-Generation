Page;Text
13;The AIDA would require that appropriate measures be put in place to identify, assess, and mitigate risks of harm or biased output prior to a high-impact system being made available for use
13;Human Oversight means that high-impact AI systems must be designed and developed in such a way as to enable people managing the operations of the system to exercise meaningful oversight. This includes a level of interpretability appropriate to the context
13;Monitoring, through measurement and assessment of high-impact AI systems and their output, is critical in supporting effective human oversight
14;Transparency means providing the public with appropriate information about how high-impact AI systems are being used. The information provided should be sufficient to allow the public to understand the capabilities, limitations, and potential impacts of the systems
15;Safety means that high-impact AI systems must be proactively assessed to identify harms that could result from use of the system, including through reasonably foreseeable misuse. Measures must be taken to mitigate the risk of harm
15;Accountability means that organizations must put in place governance mechanisms needed to ensure compliance with all legal obligations of high-impact AI systems in the context in which they will be used
15;Businesses would be expected to institute appropriate accountability mechanisms to ensure compliance with their obligations under the Act
16;This includes the proactive documentation of policies, processes, and measures implemented
17;Businesses who design or develop a high-impact AI system would be expected to take measures to identify and address risks with regards to harm and bias, document appropriate use and limitations, and adjust the measures as needed
17;Businesses who make a high-impact AI system available for use would be expected to consider potential uses when deployed and take measures to ensure users are aware of any restrictions on how the system is meant to be used and understand its limitations
17;Businesses who manage the operations of an AI system would be expected to use AI systems as indicated, assess and mitigate risk, and ensure ongoing monitoring of the system
17;developers of general-purpose systems would need to ensure that risks related to bias or harmful content are documented and addressed
17;businesses responsible for regulated activities associated with a high-impact system would also be required to notify the Minister if a system causes or is likely to cause material harm
18;Performing an initial assessment of potential risks associated with the use of an AI system in the context and deciding whether the use of AI is appropriate
18;Assessing and addressing potential biases introduced by the dataset selection
18;Assessing the level of interpretability needed and making design decisions accordingly
19;Documenting datasets and models used
19;Performing evaluation and validation, including retraining as needed
19;Building in mechanisms for human oversight and monitoring
19;Documenting appropriate use(s) and limitations
19;Keeping documentation regarding how the requirements for design and development have been met
19;Providing appropriate documentation to users regarding datasets used, limitations, and appropriate uses
19;Performing a risk assessment regarding the way the system has been made available
19;Logging and monitoring the output of the system as appropriate in the context
19;Ensuring adequate monitoring and human oversight
19;Intervening as needed based on operational parameters
20;The Government intends to allow ample time for the ecosystem to adjust to the new framework before enforcement actions are undertaken
20;In addition to administration and enforcement of the Act, the Commissioner's work would include supporting and coordinating with other regulators to ensure consistent regulatory capacity across different contexts, as well as tracking and studying of potential systemic effects of AI systems