Text
"Data and data governance: High-risk AI systems which make use of techniques involving the training of models with data shall be developed on the basis of training, validation, and testing datasets that meet a set of quality criteria"
Record-keeping: High-risk AI systems shall be designed and developed with capabilities enabling the automatic recording of events ('logs') while the high-risk AI systems is operating. Those logging capabilities shall conform to recognised standards or common specifications.
Transparency and provision of information to users: High-risk AI systems shall be designed and developed in such a way to ensure that their operation is sufficiently transparent to enable users to interpret the system's output and use it appropriately.
"Human oversight: High-risk AI systems shall be designed and developed in such a way, including with appropriate humanâ€“machine interface tools, that they can be effectively overseen by natural persons during the period in which the AI system is in use."
"Risk management system: An assessment through internal checks for 'stand-alone' high-risk AI systems would require a full, effective and properly documented ex ante compliance with all requirements of the regulation and compliance with robust quality and risk management systems and post-market monitoring. A risk management system shall be established, implemented, documented and maintained in relation to high-risk AI systems."
"Quality management system: Providers of high-risk AI systems shall put a quality management system in place that ensures compliance with this Regulation. The provider should establish a sound quality management system, ensure the accomplishment of the required conformity assessment procedure, draw up the relevant documentation and establish a robust post-market monitoring system."
"Robustness: AI systems should be resilient against risks connected to the limitations of the system (e.g. errors, faults, inconsistencies, unexpected situations) as well as against malicious actions that may compromise the security of the AI system and result in harmful or otherwise undesirable behaviour."
"The technical solutions to address AI specific vulnerabilities shall include, where appropriate, measures to prevent and control for attacks trying to manipulate the training dataset ('data poisoning'), inputs designed to cause the model to make a mistake ('adversarial examples'), or model flaws."
Record-keeping: High-risk AI systems shall be designed and developed with capabilities enabling the automatic recording of events ('logs') while the high-risk AI systems is operating. Those logging capabilities shall conform to recognised standards or common specifications.
High-risk AI systems shall be resilient as regards attempts by unauthorised third parties to alter their use or performance by exploiting the system vulnerabilities.
"High-risk AI systems are to be accompanied by instructions for use, specifying, among other things, the 'the level of accuracy, robustness and cybersecurity referred to in Article 15 against which the high-risk AI system has been tested and validated and which can be expected, and any known and foreseeable circumstances that may have an impact on that expected level of accuracy, robustness and cybersecurity'."
Transparency and provision of information to users: High-risk AI systems shall be designed and developed in such a way to ensure that their operation is sufficiently transparent to enable users to interpret the system's output and use it appropriately.
"Conformity assessment: AI systems that create a high risk to the health and safety or fundamental rights of natural persons: in line with a risk-based approach, these high-risk AI systems are permitted on the European market subject to compliance with certain mandatory requirements and an ex-ante conformity assessment."
"Ensure regulatory coherence between the draft AI Act and legislation on cybersecurity. In particular, Article 42 of the draft AI Act sets out a presumption of conformity with cybersecurity requirements for high-risk AI systems that have been certified or for which a statement of conformity has been issued under a cybersecurity scheme pursuant to Regulation (EU) 2019/881 (the Cybersecurity Act)"
"Risk management system: An assessment through internal checks for 'stand-alone' high-risk AI systems would require a full, effective and properly documented ex ante compliance with all requirements of the regulation and compliance with robust quality and risk management systems and post-market monitoring. A risk management system shall be established, implemented, documented and maintained in relation to high-risk AI systems."
"ISO/IEC 31000 is a framework for risk analysis and the management of risk analysis systems. At a more detailed level, tools for vulnerability analysis (e.g. ETSI TS 102 165-1) may apply, as well as runtime analysis tools. Many development environments will perform both static and dynamic tests on software that allow risks in the codebase to be identified. The suite of measures should operate in concert."
"High-risk AI systems are to be accompanied by instructions for use, specifying, among other things, the 'the level of accuracy, robustness and cybersecurity referred to in Article 15 against which the high-risk AI system has been tested and validated and which can be expected, and any known and foreseeable circumstances that may have an impact on that expected level of accuracy, robustness and cybersecurity'."
"Data and data governance: High-risk AI systems which make use of techniques involving the training of models with data shall be developed on the basis of training, validation, and testing datasets that meet a set of quality criteria"
"The requirements here address data quality, which is key to secure data feeds, processing and outputs. Data quality can be reinforced by the use of tools that verify the source of data and the integrity of data (i.e. to prove that data have not been manipulated between source and sink), and by limiting access to data."