Page of the document;text
31;It's important to assess what aspects of your system could leak information about your model's architecture (eg, its required input shape) and weights to an adversary. An example of this is using a web-based front end to resize images to the input size of the model. This is likely to include pre-processing or model output processing steps. Ensure these and your model are appropriately protected for your security requirements.
31;Unless further detail is needed by another process or the end user, the model should be limited to providing the required output or prediction. This limits the potential of adversarial attacks by obscuring accuracy scores and weights, etc. (Teams should be aware that certain adversarial techniques allow forms of model inference or evasion based only on the model output, although this is technically more challenging for an adversary.)
31;You may still want to consider access controls here (eg, role-based access control), enabling different levels of insight depending on user authorisation.
31;How you protect access to hardware and software will depend on your deployment infrastructure. It's important to follow the guidance relevant to your specific infrastructure.
31;Make sure you have the right expertise for protecting the parts of your deployment infrastructure and your team understands which parts of the system need to be protected or hidden from adversaries.
32;Monitor and log your users' inference requests/queries, implementing an alert system for anomalous behaviour if required.
32;Understanding how users are querying your model and flagging unusual behaviour for investigation can help you identify and prevent attacks.
32;Many attacks, including model inversion, membership inference and denial of service, are conducted by querying a model repeatedly, often at a much faster rate than a typical user. Repeated querying is also used in the creation of adversarial examples to be used in model evasion attacks.
32;Collecting and monitoring information representing both the content of queries and their metadata (for example, frequency and patterns querying) can help identify anomalous activity. You should understand what expected inputs to your model look like and use that information to spot unusual inputs that may be worth investigating further. In the event of an attack or accidental system failure, accurate logs of user activity provide a basis for a post-attack investigation.
33;Follow appropriate guidance when applying logging and auditing of logs.
33;Consider automated monitoring of user activity and logs. Understanding how suspicious behaviour might look different to expected behaviour in event logs is the foundation here.
33;The criteria for 'suspicious' behaviour will be different for each system and you should use your understanding of your system and its intended use, to develop the criteria. This should be clear from the start of the development process and include an understanding of the expected input space.
33;Flagged behaviour should be retained for audit and reviewed regularly for evidence of suspicious behaviour.
33;The actions your system takes in response to detecting unusual behaviour will vary. Common responses may include: limiting (throttling) the rate of queries that can be sent to your model and implementing appropriate pre-processing, to heighten the threshold for an adversarial user developing an opaque box attack.
33;When an alert has been raised or an investigation has concluded that a cyber security incident has occurred, report to the relevant stakeholders and authorities.
33;Use humans in the loop to investigate what automated processes flag as unusual.
33;If user behaviour has been flagged as suspicious, or during routine audits, humans should be reviewing user behaviour and the system response to this behaviour. As part of this process, the reviewer should decide whether the user interaction was malicious or not and whether the system/model response was expected and/or correct. To be able to draw conclusions, the reviewer must have a good understanding of the system requirements and architecture.
33;When the review contains personal or sensitive data, the correct guidelines and security procedures must be followed for handling and viewing data.
34;Understand the trade-off between security during operation and the output a model provides to users.
34;During operation, providing users with visibility of a model's output can allow for quick diagnosis that a system is behaving unexpectedly. This won't always be required or even possible, but in many situations it can be beneficial, especially when the negative impact is significant.
34;The creation of many attacks on ML models relies however on the same information: a model's output for a given input. If an attacker receives a more detailed output, it can make a successful attack more likely. It's therefore desirable to find a balance between protecting key information, while still allowing for 'sanity checking'.
34;A suitable balance between transparency and security will depend on the specific system application. It's likely that your system will have different classes of user â€“ such as a normal user querying the system, an engineer servicing the system and a system admin diagnosing errors. Each user requires a different level of detail about the model's behaviour and model outputs should be tailored appropriately. This can be seen as a type of role-based access control.
35;Use access controls to vary users' access to different levels of detail from model outputs.
35;Consider what level of detail each user requires. Can you represent data in a way that allows for a user to quickly check the system's behaviour without giving easy access to detailed output information? Disguising model outputs can be beneficial when it is done in a way that converts the information to be more easily digestible by a reader. Transforming data in this way can also be beneficial for security as it presents it in a way that is easier for 'sanity checking', therefore a user is able to detect adverse behaviour more quickly.
35;Two popular implementations of access controls are role-based access controls (RBAC) and Attribute-Based Access Control (ABAC). The most appropriate implementation of access control will depend on your security requirements.
35;For user roles that require transparency but aren't necessarily fully trusted, consider displaying information in summary or graphical format rather than allowing raw values to be accessed. If access to the values is required, can these be with lower accuracy, or provided at a slower rate (ie lower frequency of queries)?
35;Limit higher fidelity access to model outputs to authorised users in limited roles, such as trusted maintainers and developers.
36;Understand if and when you're using continual learning, and the risks associated with doing so.
36;The use of continual learning (CL) can be a crucial component in helping keep a system secure as well as performant: it allows you to dynamically tackle issues like model drift and erroneous predictions, while also creating a 'moving target' for attackers. CL can come in many forms and is not always obvious. From a security point of view, you can consider CL to be any time that your model's behaviour is affected by data collected during operation (an example would be adding a new data point to a k-nearest-neighbour algorithm during its deployment).
36;Although CL can bring important security benefits, the process of taking user input and feedback to retrain a model during operation also opens a new attack vector, as you are allowing the logic and behaviour of your model to be changed by possibly untrusted sources. Retraining on an updated dataset may cause the model's decision surface to change, causing old (potentially correct) behaviours to be lost under certain circumstances. This can lead to the model 'forgetting' certain behaviours, causing a drop in performance. An attacker could harness this to deliberately induce bias or undesirable behaviour in your model, meaning you should make sure that a retrained model still behaves how you intended.
36;CL also brings complications around protecting user privacy, as it requires collecting data directly from users. It also indirectly complicates the tracking of dataset and model updates as outlined in principle 2.5 - Requirements and development: track your assets, and makes supply chains more difficult to secure.
36;You should be aware of these risks, and have systems or processes in place to ensure users can't maliciously affect your model's behaviour. This will help mitigate the risk of adversarial manipulation from continual learning and improve confidentiality for users, as you won't be collecting data that doesn't meet your requirements.
37;When collecting data from users, use techniques to ensure the data is anonymised. This allows you to gain the advantage of collecting data while protecting users' privacy, reducing security concerns.
37;Where possible, consider processing data and training locally (on the user's device) without removing the data from the device. Updates to a model can then be shared with a central version.
37;Continual learning data collection and model retraining is an important part of your machine learning Operations (MLOps) process. An effective MLOps process should be automatic using a robust, consistent and understandable framework. This will allow the tracking of data throughout the continual learning process and give the ability to follow in the case of an error or attack such as poisoning.
37;It's important to be able to identify and manage model/concept drift, where a model may start to adjust its predictions based on new training data that may move outside of the original operating criteria, or a model may begin to degrade as the data changes and exhibits new features or concepts.
38;Ensure your engineers understand what continual learning is and the associated risks. Having developers who understand this will enable them to make sensible decisions throughout system design and development.
39;When carrying out continual learning, validate updates as if they are new models or datasets. Does your model still work in the way you would expect?
39;Once a model has been retrained, it is not the same as the old model and may well give quite different results, particularly on the edges of the old model's competence. It should therefore be treated as a new model.
39;Testing processes should be in place to prevent external interactions having a negative effect on your model's behaviour, and updates may need to be rigorously tested in the same way as the original model release. The extent of this testing should be chosen depending on the specific system, and may be lower for local updates that would affect only a single instance of a model.
39;Updates to your model and dataset should be validated, tracked and documented in the same way as they are during your development process outlined in principles 2.2 and 2.5 (Requirements and development: secure your supply chain and track your assets). This allows you to monitor for anomalous updates from continual learning manipulation. You will also be able to react to attacks, for example, by having the ability to roll back in the case of collecting poisoned or mislabelled data.
40;Design your architecture to ensure that certain performance targets are achieved before a newly updated continual learning model goes into production.
40;The process of creating CL models is likely to be automatic. It's important that this process only allows new models to enter operation when given parameters/performance is met.
40;As a part of your CL data collection and model retraining process, it's important to implement tracking and filtering of the data. Filters can be placed at check points throughout the CL data collection process for sanitising and cleaning incoming data. This can prevent your model being trained on data that is unwanted or deliberately malicious.
40;Within the pipeline, consider introducing a standard set of transformations and augmentations that will standardise an input for model retraining.
40;Specific transformations, augmentations or dictionaries can be introduced over time to minimise new adversarial risks.
40;Updates made to datasets and models during continual learning should be captured in their associated metadata created during the development stage of the lifecycle.
40;Encourage manual/human testing of models and filtering of dataset updates.
40;Where automated processes are used, consider using the tracked model metadata and parameters as an alert system. Significant divergence from the original or previous update of an asset may indicate anomalous or malicious behaviour. When a metric diverges by a given amount, a process can be triggered to request human verification.
40;During CL you're collecting new data and this should be treated as such. The same security concerns that are present during the gathering of data in the development stage are also present here.