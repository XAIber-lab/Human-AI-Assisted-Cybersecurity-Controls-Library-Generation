Page;Text;Confidence
29-30;"Your team will need to test your models throughout the process to mitigate against issues such as overfitting or underfitting that could undermine your model's effectiveness once deployed. Your team should only use the test set on your best model. Keep this data separate from your models until this final test. This test will provide you with the most accurate impression of how your model will perform once deployed.";High
30;"Your team will need to evaluate your model to assess how it is performing against unseen data. This will give you an indication of how your model will perform in the real world.";Medium
31;"Once you select a final model, your team will need to assess its performance, and refine it to make sure it performs as well as you need it to. When assessing your model's performance consider: how it performs compared to simpler models, what level of performance you need before deploying the model, what level of performance you can justify to the public, your stakeholders, and regulators, what level of performance similar applications deliver in other organisations, whether the model shows any signs of bias";High
32;"Using your model in your service has three stages: 1. Integrating your model - performance-test the model with live data and integrate it within the decision-making workflow";Medium
34;"Governance in safety is important to make sure the model shows no signs of bias or discrimination. You can consider whether: the algorithm is performing in line with safety and ethical considerations, the model is explainable, there is an agreed definition of fairness implemented in the model, the data use aligns with the Data Ethics Framework, the algorithm's use of data complies with privacy and data processing legislation";High
34-35;"Purpose, Governance in purpose makes sure the model is achieving its purpose/business objectives. You can consider whether: the model solves the problem identified, how and when you will evaluate the model, the user experience aligns with existing government guidance";Medium
35;"Governance in accountability provides a clear accountability framework for the model. You can consider: whether there is a clear and accountable owner of the model, who will maintain the model, who has the ability to change and modify the code";High
35;"Governance in testing and monitoring makes sure a robust testing framework is in place. You can consider: how you will monitor the model's performance, who will monitor the model's performance, how often you will assess the model";High
35;"Public narrative, Governance in public narrative protects against reputational risks arising from the application of the model";Low
35;"Quality assurance, Governance in quality assurance makes sure the code has been reviewed and validated. You can consider whether: the team has validated the code, the code is open source";Medium
36;"Risk: Project shows signs of bias or discrimination, Mitigation: Make sure your model is fair, explainable, and you have a process for monitoring unexpected or biased outputs";High
36;"Risk: Data use is not compliant with legislation, guidance or the government organisation's public narrative, Mitigation: Consult guidance on preparing your data for AI";Medium
36;"Risk: Security protocols are not in place to make sure you maintain confidentiality and uphold data integrity, Mitigation: Build a data catalogue to define the security protocols required";High
36;"Risk: You cannot access data or it is of poor quality, Mitigation: Map the datasets you will use at an early stage both within and outside your government organisation. It's then useful to assess the data against criteria for a combination of accuracy, completeness, uniqueness, relevancy, sufficiency, timeliness, representativeness, validity or consistency";High
36;"Risk: You cannot integrate the model, Mitigation: Include engineers early in the building of the AI model to make sure any code developed is production-ready";Medium
36;"Risk: There is no accountability framework for the model, Mitigation: Establish a clear responsibility record to define who has accountability for the different areas of the AI model";High
38-39;"AI ethics is a set of values, principles, and techniques that employ widely accepted standards to guide moral conduct in the development and use of AI systems";Low
39;"The main ways AI systems can cause involuntary harm are: misuse - systems are used for purposes other than those for which they were designed and intended, questionable design - creators have not thoroughly considered technical issues related to algorithmic bias and safety risks, unintended negative consequences";Medium
40;"ethically permissible - consider the impacts it may have on the wellbeing of affected stakeholders and communities";Medium
40;"fair and non-discriminatory - consider its potential to have discriminatory effects on individuals and social groups, mitigate biases which may influence your model's outcome, and be aware of fairness issues throughout the design and implementation lifecycle";High
40;"worthy of public trust - guarantee as much as possible the safety, accuracy, reliability, security, and robustness of its product";Medium
40;"justifiable - prioritise the transparency of how you design and implement your model, and the justification and interpretability of its decisions and behaviours";High
40-41;"respect the dignity of individuals, connect with each other sincerely, openly, and inclusively, care for the wellbeing of all, protect the priorities of social values, justice, and public interest";Low