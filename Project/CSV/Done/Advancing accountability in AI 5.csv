Page;Text;Confidence
41;"Design an end-to-end explainable AI (XAI) framework, from DataOps to inference";Medium
41;"Establish processes to document the entire AI system lifecycle to enhance transparency";High
41;"Consider using existing documentation tools, which may be relevant to one or several lifecycle phases: e.g. Google Model Cards, Microsoft Datasheets for Datasets, Meta System Cards, etc.";High
41;"Perform exploratory data analysis using visualisation tools to understand datasets";Medium
41;"Standardise dataset and model descriptions: frameworks to drive higher data quality standards";High
41;"Summarise datasets: explain data through clusters";Low
41;"Document model inputs in design documentation";High
42;"DevOps: combine software development (Dev) and IT operations (Ops)";Medium
42;"CI/CD: practice continuous development, integration, delivery, and deployment";Medium
42;"Secure-by-design: use good design principles, tools, and mindsets that make security an implicit result";High
42;"Assess conformity with consumer safety regulations";High
42;"Incorporate a data protection and secure integration plan into technical documentation";High
42;"Review vendor documentation and rigorously scan for vulnerabilities";High
42;"Verify piecewise-linear neural networks as a mixed integer program for model robustness evaluation";Medium
42;"Propose a provable guarantee of robustness as an alternative to heuristic defences";Medium
42;"Dataset shift: analyse model robustness keeping the original data";Medium
42;"Processing speed and use of computing resources can evolve during an AI system's lifespan";Medium
42;"Usage and performance of computing resources should be monitored to guarantee reasonable processing times and keep an eye on costs and energy consumption";High
42;"The increasingly central role of environmental considerations in companies, governments, and societies makes an AI system's energy consumption and carbon footprint evaluations essential";Medium
43;"Both known and unknown AI risks should be anticipated to prevent harm";High
43;"Unknown risks might include risks to robustness (e.g. breakdown); security (e.g. hacks); secondary uses or misuses of a system, including use of pre-packaged coding for malicious purposes; psychological and social impact; and reputational risks";High
43;"Risk and impact assessments can be conducted to identify risks and design mitigation strategies before, during, and after deployment";High
43;"One approach to identifying unknown risks is known as 'red teaming', which refers to systematic and controlled attempts to probe and expose flaws and weaknesses in a system, process, or organisation";High
43;"Code versioning: e.g., Git (Github); Mercurial (BitBucket)";Medium
43;"Reproducibility: tools that allow reproducibility of models, e.g., Binder; Docker; Kubernetes";Medium
43;"Automated testing: e.g., Travis CI; Scrutinizer CI";High
43;"Trap-based monitoring sensors: an efficient way to infer Internet threat activities";High
43;"Use of dashboards to monitor performance, errors and suggest courses of action";High
44;"Monitoring and reviewing risks and steps taken to treat them contributes to the correct functioning of an AI system";High
44;"Given the evolving nature of AI systems and the environments in which they operate, monitoring should be continuous, rather than a one-off activity, and happen at all stages of the risk management process";High
45;"Users of AI systems should be able to report controversies, incidents, or issues, either regarding inaccurate predictions, unfair outcomes, or unexpected or undesirable behaviour";High
45;"Human-in-the-loop: Human-in-the-loop mechanisms monitor AI models at different stages of their development and use, including by testing and validating outputs, responding to system alerts during deployment";High
46;"Logs: basic logging of metrics helps create conditional workflows, for example, by setting thresholds for accuracy or fairness outside of which alerts are triggered and automated or manual actions taken";High
46;"Visual reports: logs can be represented visually through tables, charts or diagrams";Medium
46;"Live dashboards: visual reports can be automated into live or real-time dashboards to enable users to interactively explore model capabilities and shortcomings";High
46;"Monitoring and review frequency should be appropriate to the application and context of each AI system";High
46;"Indicators should cover all relevant known – and unknown, if possible – technical and non-technical risks";High
47;"Seven access levels enable auditing and review at varying degrees of scrutiny";Medium
47;"Different access levels could allow for auditing and review tailored to a specific AI application and its context, including commercial sensitivities and legal and ethical requirements";High
48;"Assessing model performance is challenging in the absence of information about the outputs of the system";Medium
49;"This access level allows the reviewer to assess how stable system performance is and evaluate the quality of explanations being provided";Medium
50;"Whether the AI system is built in-house or by a third party, documentation and logs should follow the system throughout the supply chain; that is, each involved party or actor – from the developer to the vendor to the deployer – might need to conduct their own assessments and document the actions taken to manage risks";High
50;"The broader outcome of a risk assessment and management process is to protect human rights and democratic values, improve confidence in the AI system, and ensure it is trustworthy";High
50;"A trail or log documenting the steps, decisions and actions, and their rationale during the risk management process provides the basis for communication and consultation";High
50;"Verifying and communicating publicly whether an AI system conforms to regulatory, governance, and ethical standards after assessing and treating risks is crucial";High