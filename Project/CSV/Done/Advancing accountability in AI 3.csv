page;text;confidence
page 22;Accountability in the AI ecosystem should be shared by everyone directly and indirectly involved in or affected by the development or use of an AI system. All actors should manage risks based on their roles, the context, and following the state-of-the-art;high
page 22;Using an AI system in ways that differ from its intended use (e.g. secondary uses or misuses) might require a re-assessment of the context and risks;medium
page 22;Taking a balanced approach to managing risks without violating human rights or stifling innovation is key. The risks of AI should be balanced against the risks of not using AI in contexts where it can provide crucial benefits and insights;medium
page 23;Actors in the "Verify and validate" phase include developers, modellers, and auditors involved in the testing and tuning of models to assess and improve performance across various dimensions and considerations;medium
page 23;The users of an AI system or application are individuals or organisations that use it to achieve a specific task or objective. They include procurers who acquire AI models, products, or services from a third party, developer, vendor, or contractor;medium
page 23-24;Users should be accountable for the legal and ethical use of an AI system, and have a role in monitoring and reporting its risks and impacts;high
page 23-24;Continuous re-skilling and up-skilling is crucial in this regard. Stakeholders encompass all organisations and individuals affected by AI systems, directly or indirectly;low
page 24;Incident-reporting mechanisms and awareness-raising campaigns can help stakeholders monitor downstream risks, negative externalities, and risks that materialise despite a system working as intended;high
page 24;Actors in the "Operate and monitor" phase include system operators in the continuous assessment of the system's outputs and impacts (both intended and unintended) against its objectives and the ethical considerations of its operation;high
page 26;Once the scope, context, actors and criteria for an AI system are defined, it is important to assess the risks it poses and which could result in the AI system failing to meet its trustworthiness objectives. This process consists of identifying or discovering risks, analysing the mechanisms by which those risks may occur, and evaluating their likelihood of occurring as well as their severity;high
page 26;Sustainability: the computing power ("compute") used to train AI models has grown exponentially in recent years, affecting workloads and energy consumption at data centres;low
page 26-27;Higher energy consumption should thus be weighed against the benefits of these systems;low
page 27;AI should be developed based on human-centred values, including human rights, fundamental freedoms, equality, fairness, the rule of law, social justice, data protection and privacy, and consumer rights and commercial fairness;medium
page 27;Promote "rights and values alignment" in AI systems (i.e. their design with appropriate safeguards), including capacity for human intervention, oversight, and redress, as appropriate to the context;high
page 28;Data-governance mechanisms should be in place to ensure the quality and integrity of the data used to train the model; its relevance in the system's deployment context; its access protocols; and the model's capacity to process data in a manner that protects privacy and sensitive information;high
page 29;AI systems should respect privacy and data protection throughout their lifecycle;high
page 29;Data access and disposal protocols outlining who can access and delete data, and under which circumstances, should also be put in place;high
page 29;The security and privacy of an AI model can be assessed based on: (1) the access level a malicious actor might have, from "black box" (e.g. no knowledge about the model) to "full transparency" (e.g. full information about the model and its training data); (2) the phase in which an attack might happen (e.g. during training or inference); and (3) whether passive (e.g. "honest but curious") or active (e.g. fully malicious) attacks are possible;high
page 29;Data protection impact assessment is the standard procedure to assess risks;high
page 30;Advanced privacy-enhancing technologies (e.g. homomorphic encryption, secure multi-party computation, and differential privacy) and novel approaches to training (e.g. using data combined from multiple organisations, federated machine learning) can be leveraged to protect an AI system and increase its privacy;high