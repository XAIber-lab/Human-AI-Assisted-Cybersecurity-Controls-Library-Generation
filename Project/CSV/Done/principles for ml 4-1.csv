Page of the document;text
31;Strengthen your infrastructure. It's important to assess what aspects of your system could leak information about your model's architecture (eg, its required input shape) and weights to an adversary. An example of this is using a web-based front end to resize images to the input size of the model. This is likely to include pre-processing or model output processing steps. Ensure these and your model are appropriately protected for your security requirements.
31;Unless further detail is needed by another process or the end user, the model should be limited to providing the required output or prediction. This limits the potential of adversarial attacks by obscuring accuracy scores and weights, etc. (Teams should be aware that certain adversarial techniques allow forms of model inference or evasion based only on the model output, although this is technically more challenging for an adversary.)
31;You may still want to consider access controls here (eg, role-based access control), enabling different levels of insight depending on user authorisation.
31;How you protect access to hardware and software will depend on your deployment infrastructure. It's important to follow the guidance relevant to your specific infrastructure.
31;Make sure you have the right expertise for protecting the parts of your deployment infrastructure and your team understands which parts of the system need to be protected or hidden from adversaries.
32;Monitor and log your users' inference requests/queries, implementing an alert system for anomalous behaviour if required.
32;Understanding how users are querying your model and flagging unusual behaviour for investigation can help you identify and prevent attacks.
32;Collecting and monitoring information representing both the content of queries and their metadata (for example, frequency and patterns querying) can help identify anomalous activity. You should understand what expected inputs to your model look like and use that information to spot unusual inputs that may be worth investigating further. In the event of an attack or accidental system failure, accurate logs of user activity provide a basis for a post-attack investigation.
33;Follow appropriate guidance when applying logging and auditing of logs.
33;Consider automated monitoring of user activity and logs. Understanding how suspicious behaviour might look different to expected behaviour in event logs is the foundation here.
33;The criteria for 'suspicious' behaviour will be different for each system and you should use your understanding of your system and its intended use, to develop the criteria. This should be clear from the start of the development process and include an understanding of the expected input space.
33;Flagged behaviour should be retained for audit and reviewed regularly for evidence of suspicious behaviour.
33;The actions your system takes in response to detecting unusual behaviour will vary. Common responses may include: limiting (throttling) the rate of queries that can be sent to your model and implementing appropriate pre-processing, to heighten the threshold for an adversarial user developing an opaque box attack.
33;When an alert has been raised or an investigation has concluded that a cyber security incident has occurred, report to the relevant stakeholders and authorities.
33;Use humans in the loop to investigate what automated processes flag as unusual.
33;If user behaviour has been flagged as suspicious, or during routine audits, humans should be reviewing user behaviour and the system response to this behaviour. As part of this process, the reviewer should decide whether the user interaction was malicious or not and whether the system/model response was expected and/or correct. To be able to draw conclusions, the reviewer must have a good understanding of the system requirements and architecture.
33;When the review contains personal or sensitive data, the correct guidelines and security procedures must be followed for handling and viewing data.
34;Understand the trade-off between security during operation and the output a model provides to users.
34;Consider, for example, whether an end-user needs to know the confidence level of your model's predictions to multiple decimal places, or whether providing a low-fidelity summarising output (eg, simple classification) would be sufficient, retaining more detailed information for internal audit only.
34;You may also consider the way that information is provided at each level of user access. For example, displaying model outputs to a user via a dashboard whist restricting programmatic access could still provide benefits to the user while greatly increasing the difficulty of an attacker's task.
35;Use access controls to vary users' access to different levels of detail from model outputs.
35;Consider what level of detail each user requires. Can you represent data in a way that allows for a user to quickly check the system's behaviour without giving easy access to detailed output information? Disguising model outputs can be beneficial when it is done in a way that converts the information to be more easily digestible by a reader. Transforming data in this way can also be beneficial for security as it presents it in a way that is easier for 'sanity checking', therefore a user is able to detect adverse behaviour more quickly.
35;Two popular implementations of access controls are role-based access controls (RBAC) and Attribute-Based Access Control (ABAC). The most appropriate implementation of access control will depend on your security requirements.
35;For user roles that require transparency but aren't necessarily fully trusted, consider displaying information in summary or graphical format rather than allowing raw values to be accessed. If access to the values is required, can these be with lower accuracy, or provided at a slower rate (ie lower frequency of queries)?
35;Limit higher fidelity access to model outputs to authorised users in limited roles, such as trusted maintainers and developers.
36;Understand if and when you're using continual learning, and the risks associated with doing so.
37;When collecting data from users, use techniques to ensure the data is anonymised. This allows you to gain the advantage of collecting data while protecting users' privacy, reducing security concerns.
37;Where possible, consider processing data and training locally (on the user's device) without removing the data from the device. Updates to a model can then be shared with a central version.
37;Continual learning data collection and model retraining is an important part of your machine learning Operations (MLOps) process. An effective MLOps process should be automatic using a robust, consistent and understandable framework. This will allow the tracking of data throughout the continual learning process and give the ability to follow in the case of an error or attack such as poisoning.
37;It's important to be able to identify and manage model/concept drift, where a model may start to adjust its predictions based on new training data that may move outside of the original operating criteria, or a model may begin to degrade as the data changes and exhibits new features or concepts.
38;Ensure your engineers understand what continual learning is and the associated risks. Having developers who understand this will enable them to make sensible decisions throughout system design and development.
39;When carrying out continual learning, validate updates as if they are new models or datasets. Does your model still work in the way you would expect?
39;Once a model has been retrained, it is not the same as the old model and may well give quite different results, particularly on the edges of the old model's competence. It should therefore be treated as a new model.
39;Updates to your model and dataset should be validated, tracked and documented in the same way as they are during your development process outlined in principles 2.2 and 2.5 (Requirements and development: secure your supply chain and track your assets). This allows you to monitor for anomalous updates from continual learning manipulation. You will also be able to react to attacks, for example, by having the ability to roll back in the case of collecting poisoned or mislabelled data.
40;Design your architecture to ensure that certain performance targets are achieved before a newly updated continual learning model goes into production.
40;As a part of your CL data collection and model retraining process, it's important to implement tracking and filtering of the data. Filters can be placed at check points throughout the CL data collection process for sanitising and cleaning incoming data. This can prevent your model being trained on data that is unwanted or deliberately malicious.
40;Within the pipeline, consider introducing a standard set of transformations and augmentations that will standardise an input for model retraining.
40;Specific transformations, augmentations or dictionaries can be introduced over time to minimise new adversarial risks.
40;Updates made to datasets and models during continual learning should be captured in their associated metadata created during the development stage of the lifecycle.
40;Encourage manual/human testing of models and filtering of dataset updates.
40;Where automated processes are used, consider using the tracked model metadata and parameters as an alert system. Significant divergence from the original or previous update of an asset may indicate anomalous or malicious behaviour. When a metric diverges by a given amount, a process can be triggered to request human verification.
40;During CL you're collecting new data and this should be treated as such. The same security concerns that are present during the gathering of data in the development stage are also present here