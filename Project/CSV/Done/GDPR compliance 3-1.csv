"Page of the document;text"
21;It must be taking into accout that to ground the lawfulness of a processing in a legitimate interest requires from the organization acting as a controller a higher degree of commitment, formality and competence. It requires a careful assessment that their legitimate interests prevail over the possible impact on the rights, freedoms and interests of the data subjects, considering, among other things, eventual compensatory measures arising from the need to keep the processing activities continuously supervised, implement a high degree of accountability and stricter design default privacy measures or the adoption of best practices such as giving an opt-out option to data subjects
21;If the processing activities are based on the legitimate interest, it is not necessary to collect the consent of the data subject, but information obligations provided by articles 13 and 14 of the GDPR persist.
22;The first layer should include: • The identity of the of the data controller or their representative. • The purpose of the processing. • The possibility of exerting the rights included in articles 15 to 22 of the GDPR. • If the processing involves profiling or automated decisions: • This circumstance must be clearly disclosed. • Informing of their right to oppose making any automated individual decisions pursuant to article 22 of the GDPR. • Relevant information on the implemented logic. • Relevance and foreseen consequences of the relevant processing for the data subject.
23;Complying with this obligation by making a technical reference to the algorithm implementation may be obscure, confuse or excessive and leading to information fatigue. However, sufficient information must be provided to understand the behaviour of the relevant processing. Although it shall depend on the type of AI component used, an example of the type of information which may be relevant for the data subject would be: • Detailed information about the subject's data used for decision-making regardless of the category, especially regarding with how old the subject's data under processing are. • The relative importance or weight of each data category in the decision making. • The quality of training data and the type of patterns used. • Profiling activities conducted and their implications. • Error or precision values, according to the appropriate metrics used to measure the eligibility of the inference. • Whether qualified human supervision is involved or not. • Any reference to audits, especially on the possible deviation of inference results, as well as certification or certifications performed on the AI system. For adaptive systems or evolutive systems, the last audit conducted. • If the AI system includes information referring to identifiable third-data subjects, prohibition of processing such information without legitimisation and of the consequences of doing so.
23;Any data controllers that use AI-based solutions to process personal data, conduct profiling or make automated decisions must be aware that the data subject have rights related to the protection of their personal data which must be attended to.
24;The right of access must be executed by the controller of each life cycle stage of the AI-based solution involving personal data. This includes any training data which may be included in the AI components and which are susceptible to be retrieved by the controller who operates the relevant AI-based solution.
24;The right to erasure involves a proactive attitude by the data controller in order to, as established by Whereas 39, guarantee that data are erased when they are no longer necessary for the purpose of the processing, and, particularly, for including procedures for the period review of the relevant datasets, and terms for its erasure.
24;Data collected for the training stage, in compliance with the aspects set forth by article 11 of the GDPR and with the principle of data minimisation, must be cleansed or sanitized from all information which is not strictly necessary to train the model.
24;Once the training stage of an AI system is completed, the organization must execute its removal unless they are needed for the system fine-tuning or assessment, or they are used for other purposes which are compatible with the original purposes as per the provisions of article 6.4 of the GDPR. In any case, the data minimisation principles must be considered.
25;Particularly, when the AI-based solution is released to controllers and natural persons, if this includes data from data subjects: • They must be either erased, or to assess that it is partially or entirely impossible because it would impair the systems, • The relevant legal grounds to disclose data to a third party must be established, especially when data of special categories are included, • The data subjects must be informed (as stated above), • Proof must be offered that the relevant privacy by default and by design measures have been implemented (most specifically, data minimization) and, • Depending on the risk involved for data subjects and the volume or categories of data, to carry out a privacy impact assessment.
25;Blocking constitutes an obligation of the data controller whose sole purpose is to provide a response to any possible liabilities arising from the data processing, for the purposes of obtaining evidence of any possible non-compliance and exclusively during their validity term.
25;Article 32 of LOPDGDD establishes blocked data as the status of data kept outside the processing scope, applying any technical or organizational measures that prevent any type of process, including visualization, except for the purposes of making such data available to the relevant courts or judges, the public prosecutor or the competent Public Administration, especially the data protection authorities.
25;Therefore, the need to include the above measures to block any data related to the inference process (or at least inputs and results) which may be needed to respond a request or claim by the relevant data subject must be considered as a requirement when designing the processing. These mechanisms are also related to the log files that shall be discussed below.
25;The data controller shall be obliged to respond to the exercise by data subjects of their right to rectification, especially when this right arises from inferences and profiles created by the relevant AI-based solution.
26;Article 20 of the GDPR establishes that, when the processing is carried out by automated means the data subject has the right to request any data that has provided to the relevant controller, get it in an structured, common-use, machine-readable format, and to transfer it to another data controller, when the legitimating grounds for the processing is either the data subject's consent or the processing is needed to execute an agreement.
26;Any data controller who includes an AI component must assess and document whether their processing must provide portability, considering the provisions of the aforementioned article 20. In such case, the requirement of portability must be considered from the earlier stages of conceptualization and design, as well as in the selection of the AI component by the AI component developers.
26;Article 20.2 of the GDPR establishes the right to have data directly transferred from one data controller to another, but only when it is technically feasible. In case that there are limitations to the right of portability, informing users beforehand of such limitations is considered an exercise of transparency.
26;The GDPR guarantees the right not to be submitted to automated decisions included profiling when: • There is no human intervention. In order to consider that there is human intervention, the relevant decision must be supervised by a competent person authorised to revert such decision by means of a significant action and not one purely symbolic. • There are legal consequences derived from such decisions. • Or the data subject is similarly and significantly affected.
27;When the legal grounds for processing is explicit consent, the controller must design the processing in such a way that it protects the free choice of users. This is done, first, by providing viable and equivalent alternatives to automated solutions at the time when their consent is required. Besides, it is guaranteed that if the data subject chooses not to be subject to automated decisions, the decision concerning such data subject shall not be biased and goes against the data subject's interests. If the above conditions are not met, consent may not be considered to have been freely given. These alternatives must be implemented from the processing design stage.
27;As best practices, and beyond any requirement arising from data protection, human supervision may be an option which may be chosen within AI-based data processing, and in general with regard to automated decisions. The "dead man switch" approach must be avoided in system design: human users must have the option to ignore the algorithm at a given time in all cases, and to document the situations in which this course is privileged. For this reason, it is recommended to document any incidence or any challenges to automated decisions by the relevant data subject, so that its analysis allows to detect situations which require human intervention, because the processing is not operating as expected.
28;When processing personal data and it is proportionally to the processing activities, the organization must adopt a Data Protection Policy as established by article 24 of the GDPR. This policy should be integrated in their quality policy, their secure information systems and their decision-making policy (among others).
28;Although the GDPR does not specifically establish which accountability elements need to be compulsorily implemented or how to implement them, but merely provides a highly flexible framework in order to allow very different processing approaches to be compliant with the relevant standard, GDPR lays down that such measures must be selected according a risk based approach (RBT) which specifically considers the risks posed by personal data processing and profiling with regard to their rights and liberties.
28;RBT includes two fundamental stages: a first stage consisting on identifying threats and assessing the existing inherent risk level, and a second stage consisting in managing this risk by means of appropriate and proportional technical and organizational measures in order to remove or at least mitigate such risk, with the purposes of reducing the probability of impact of the identified threat. Once the chosen measures have been implemented, the remaining residual risk must be assessed and controlled.
28;In order to determine the risk and to establish the appropriate measures to manage it, the processing must be assessed and divided into stages. Therefore, the special requirements and risks of each stage must be managed from the data protection point of view.
28;In order to determine the level of risk of a processing, which is based on or includes stages that feature an AI component, it must be taking into account: • Any risks arising from the processing itself, in particular that derived from the bias present in decision-making systems and the natural persons discrimination (algorithmic discrimination). • Any risks arising for processing with regard to social context and collateral effects which may be derived from the processing, even those that are indirectly related to the purpose of the processing.
29;The PIA is an obligation laid down by the GDPR for high risk processing. This obligation requires to go further from the mere risk management related to the processing, and it requests an additional seriousness, accountability, at the time to manage such risk.
29;The need for each data controller to perform a data protection impact assessment is laid down in Article 35 of the GDPR when, as established in Paragraph 1, "the processing, is likely to result in a high risk to the rights and freedoms of natural persons".
29;More precisely, but without limitation, as established in Article 35.3 a, it is necessary to perform a PIA whenever a profiling is being carried out that is based on automated processing (but not necessarily exclusively automated processing), and decisions are making that produce legal effects concerning the natural person or significantly affect the natural person.
29;The PIA is performed before the effective processing of the personal data, it means, before the actual operation of the processing. This means, inter alia, that the PIA must not be carried out during the processing validation phase, which includes the AI component, or simultaneously with the operation phase. Therefore, the validation must be performed before designing/selecting and implementing the AI solution for a processing so that the assessment of the privacy requirements can be made in advance and the privacy by design measures and the privacy by default measures may be effectively implemented.
29;Article 35.2 of the GDPR lays down the obligation of the controller to seek advice from the Data Protection Officer, when a Data Protection Officer has been appointed.
29;It is likewise important to take into account that Article 35.9 of the GDPR establishes that, when applicable, the controller shall seek the opinion of the data subjects.
29;In the case of AI solutions, this opinion is particularly important to understand not only the technology itself, but the precise context where this technology is to be used and the specific risks for the rights and freedoms of the data subjects. Then it is a good practice to extend such consultation to all the stakeholders, including human operators dealing or supervising the AI results and third-data subjects affected by the processing.
29;More precisely, when the processing that is carried out in an automated way creates profiles and makes decisions, all such decisions must be identified in all phases of the processing, and the operating parameters must be analysed, such as, for example, error rates, and the effects that such decisions have on the data subjects must be carefully analysed.
30;The PIA must be documented and, when a high residual risk is detected, it must be subject to consultation before the supervisory authority under the conditions established in Article 36 of the GDPR.
30;A PIA must result in the adoption of a series of specific measures to manage risk, some of them aimed at reinforcing the obligations of fulfilment depending on such risk and affecting: • The conception of the processing itslef, its division in phases, procedures, technologies and extent of the processing. • The implementation of privacy by default measures and privacy by design measures in the processing pursuant to the principles of: o Minimising the amount of data that is being processed, both in terms of volume of information gathered and the size of the population that is the subject of the analysis, as well as throughout the different phases of the processing. o Aggregating the personal data to the extent possible, so as to reduce the level of detail that can be obtained as much as possible. o Concealing personal data and their interrelations to limit their exposure and to avoid their visibility before non-data subjects. o Separating the contexts of the processing to hinder the correlation of independent sources of information as well as the possibility to infer information. o Improving the information for the data subjects, in terms of time and the specific channels, about the characteristics and the legal grounds for the processing, thus enhancing transparency and allowing the data subjects to adopt informed decisions on the processing of their data. o Providing means for the data subjects so that they can control the way their data are collected, processed, used and released to third parties by implementing mechanisms that are adapted to the level of risk that enables them to exert their rights in terms of data protection. o Complying with a privacy policy that is consistent with the legal obligations and requirements imposed by the regulation. o Proving, as the accountability principle states, that the data protection policy that is applicable is being fulfilled, as well as all other legal requirements and obligations imposed by the Regulation, with regard to the data subjects and the supervisory authorities. This includes a dynamic audit of the output/results of the processing, assessing the accuracy divergences and deviations, including the algorithms that have been executed, in order to adopt corrective measures. Among such measures are to cancel data and to document in detail of the analysis performed and the measures adopted. • The identification of the security requirements to reduce the risk with regard to privacy. • The adoption of specific measures to implement a governance system of personal data enabling to provide evidence and traceability of the fulfilment of the principles, rights and guarantees in order to manage the risk