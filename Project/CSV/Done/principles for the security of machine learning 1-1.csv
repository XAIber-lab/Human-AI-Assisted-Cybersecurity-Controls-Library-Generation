Page;Text
7;Ensuring that your ML practitioners, data scientists and software developers are familiar with the inherent vulnerabilities and failure modes in ML workflows and algorithms. They should follow best practices to reasonably mitigate the threats associated with them.
7;Security is considered a key part of your ML project and integrated into your workflows. Your team members at all levels understand this.
8;Enable and encourage a positive security culture and accountability by ensuring team members have the relevant security knowledge. Support this by providing training on the threats that are unique to ML components, where knowledge gaps are identified.
8;Encourage best security practices by making sure security reviews are integrated into your system lifecycle processes. Make people accountable for these processes and use automation where appropriate.
9;What does your system look like if the ML component fails? Identify harm done virtually, physically and reputationally. How will you identify if your system has 'failed'?
9;You understand how your ML model should perform, and design processes to identify and/or correct if it fails, either unintentionally or because of an adversary.
9;You understand the implications of attacks on your ML model and the effects on wider system behaviour.
9;You have sufficient expertise for your whole system design and application, not just AI/ML knowledge.
9;You understand the wider consequences if your system is compromised, such as lower user confidence and reputational damage to both your system and organisation.
10;Consider how the design of your wider system can be used to limit the impact if the confidentiality, integrity or availability of the ML component (eg, model) is compromised. This may be baked in natively to the wider system design (including humans in the loop), or may require you to make design decisions based on ML component's limitations and your risk appetite.
10;Create system models for worst-case scenarios and modelling the effects of integrity attacks.
10;Ensure your design team and operational end users have experience across the range of disciplines or domains required for the system design process.
8;As research in this field is fast moving, you should make sure that practitioners receive the right support to keep up to date with the latest attacks and vulnerabilities for ML techniques, or any other common vulnerabilities and exposures on applicable assets (such as models and datasets) or relevant software. A method capturing ML-related objective is cyber exercising.
8;If teams are aware of and accountable for security-related responsibilities through the lifecycle, it encourages a positive security culture. Depending on your specific development process, automated security testing may be an option, although it's still important to have a security-centred culture.
10;Encourage a cross-discipline culture that recognises the need to work collaboratively during the design process. Put in place processes and procedures to share knowledge and experience where disciplines meet and overlap. This is particularly important when highlighting the limitations of the ML components with the wider system designers. Ensure ML experts work collaboratively with system designers and formalise methods of exchanging information.
6;ML practitioner: 1.1 Be aware of ML security vulnerabilities and model failure modes, 1.2 Consider failure modes, 1.3 Understand what attackers can do with information about your system
6;Senior decision maker: 1.1 Be aware of ML security vulnerabilities and model failure modes, 1.3 Understand what attackers can do with information about your system
6;IT security professional: 1.1 Be aware of ML security vulnerabilities and model failure modes, 1.2 Consider failure modes, 1.3 Understand what attackers can do with information about your system
6;ML practitioner: 2.1 Consider your system's vulnerability to known ML threats, 2.2 Consider digital and physical supply chains risk, 2.5 Document the creation, operation and lifecycle management of assets, 2.6 Ensure your model capacity is proportionate to your requirements
6;Senior decision maker: 2.2 and 2.3 Consider digital and physical supply chains risk, 2.4 Secure your infrastructure
6;IT security professional: 2.1 Consider the vulnerability of your system to known ML threats, 2.2 and 2.3 Consider digital and physical supply chains risk, 2.4 Secure your infrastructure, 2.5 Document the creation, operation and lifecycle management of assets
6;ML practitioner: 3.2 Monitor and log your users' inference requests and queries, 3.3 Understand the trade-off between security during operation and the output a model provides to users
6;Senior decision maker: 3.2 Monitor and log your users' inference requests/queries
6;IT security professional: 3.1 Limit access to your model, 3.2 Monitor and log your users' inference requests and queries
6;ML practitioner: 4.1 Understand what continual learning is and the potential risks of using it, 4.2 Track and test continual learning models
6;Senior decision maker: 4.1 Understand what continual learning is and the potential risks of using it
6;ML practitioner: 5.1 Decommission your assets appropriately, 5.2 Collate lessons learned and share
6;Senior decision maker: 5.2 Collate lessons learned and share
6;IT security professional: 5.1 Decommission your assets appropriately, 5.2 Collate lessons learned and share