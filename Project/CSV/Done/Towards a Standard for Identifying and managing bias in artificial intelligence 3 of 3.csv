page;text;confidence
41;HCD is that of actively involving end-users and appropriate stakeholders in the process. In the context of AI, this means placing humans in the loop, not only through meaningful human control [252], but also through their active participation in the preparation, learning, and decision-making phases of AI;high
41;An HCAI approach will reduce the prospects for out-of-control technologies, calm fears of robot-driven unemployment, and diminish the threats to privacy and security. A human-centered future will also support human values, respect human dignity;medium,definition
42;AI systems may perform differently than expected once deployed, which can lead to differential treatment of individuals from different groups. A key measure to control this risk is to deploy additional systems that monitor for potential bias issues, which can alert the proper personnel when potential problems are detected.;high
42;A key consideration for the success of live monitoring for bias is the collection of data from the active user population, especially data related to user demographics such as age and gender, to enable calculation of assessment measures.;high
42;Availability of feedback channels allow system end users to flag incorrect or potentially harmful results, and seek recourse for errors or harms.;high
43;Though not without criticism [255], adverse action notices for negative consumer credit decisions, as mandated by the Equal Credit Opportunity Act and the Fair Credit Reporting Act, are an example of an explanation and appeal process;medium,definition
43;Additional appeal and override processes could include options for customers to interact with a human instead of an AI system or options to avoid similar AI-generated content in the future.;high
43;ensuring that written policies and procedures address key roles, responsibilities, and processes at all stages of the AI model lifecycle is critical to managing and detecting potential overall issues of AI system performance;high
43;Policies may: define the key terms and concepts related to AI systems and the scope of their intended impact; address the use of sensitive or otherwise potentially risky data; detail standards for experimental design, data quality, and model training; outline how the risks of bias should be mapped and measured, and according to what standards; detail processes for model testing and validation; detail the process of review by legal or risk functions; set forth the periodicity and depth of ongoing auditing and review; outline requirements for change management; and detail any plans related to incident response for such systems, in the event that any significant risks do materialize during deployment.;high
44;Clear documentation practices can help to systematically implement policies and procedures, standardizing how an organization's bias management processes are implemented and recorded at each stage.;high
44;Model documents should contain interpretable descriptions of system mechanisms, enabling oversight personnel to make informed, risk-based decisions about the system's potential to perpetuate bias.;high
44;Documentation also serves as a single repository for important information, supporting not only internal oversight of AI systems and related business processes, but also enhancing system maintenance, and serving as a valuable resource for any necessary corrective or debugging activities.;high
44;Some model documentation templates also include contact information for developers and stakeholders;medium
45;Accountability plays a critical role in governance efforts;low,definition
45;Ensuring individuals or teams bear responsibility for risks and associated harms provides a direct incentive for their mitigation.;high
45;Accountability for AI bias cannot lie on the shoulders of a single individual, which is why accountability mandates should also be embedded within and across the various teams involved in the training and deployment of AI systems.;high
45;Model or algorithmic audits can be used to assess and document such crucial accountability considerations.;medium
46;An organizational culture that encourages serious questioning of AI system designs will be more likely to identify problems before they turn into harmful incidents.;high
46;Model risk management frameworks, for example, are often systematically implemented through the so-called "three lines of defense," which creates separate teams that are held accountable for different aspects of the model lifecycle. Typically, the first line of defense focuses on model development, the second on risk management, and the third on auditing.;high
46;Developing a risk mitigation mindset, meaning a clear acceptance that incidents can and will occur, and emphasizing practical detection and mitigation once they do, can help ensure that any risks of bias are quickly mitigated in practice.;high
46;A central cultural component of effective risk management for AI bias lies in a clear acknowledgment that risk mitigation, rather than risk avoidance, is often the most effective factor in managing such risks.;high
46;This acknowledgement enables a clear triaging of risks which can enable organizations to focus finite resources on the risks of bias that are most material, and therefore most likely to cause real-world harm.;high
46;Identifying internal mechanisms for teams to share information about bias incidents or other harmful impacts from AI helps to elevate the importance of AI risks and provides information for teams to avoid past failed designs.;high