Page;Extract
7;"Part of this is technical. Independent, third-party organizations need access to AI system data and code to run technical audits or impact assessments. New York City, for example, adopted a law to require third-party bias audits of algorithms used by employers in hiring or promotions."
7;"Third parties like researchers, civil society groups, community organizations, and regulators also need access to transparency and explainability information used internally by an organization, so they can fully understand and assess the design and deployment of AI-powered biometric technologies. Industry can also embed accountability mechanisms into system design, such that human interventions are possible (e.g., in clinical settings and law enforcement settings) as necessary."
8;"Other parts of internal accountability regimes, however, are not technical. Companies developing AI tools need not only internal ethical guidelines (e.g., policies on transparency and explainability) but also human involvement in accountability structures, such as "rank-and-file employee representation on the board of directors, external ethics advisory boards, and the implementation of independent monitoring and transparency efforts." Congress, for example, has introduced bills in the past few years to mandate companies to conduct annual audits or impact assessments of AI algorithms."
8;"Implement executive and legislative actions to allow third-party auditor access to AI data and source code, as well as other transparency and explainability information, for the purposes of external researcher, civil society, and regulator assessments."
8;"Consider best practices that could be recommended to industry for embedding accountability mechanisms to test the outcome or results of its AI systems."