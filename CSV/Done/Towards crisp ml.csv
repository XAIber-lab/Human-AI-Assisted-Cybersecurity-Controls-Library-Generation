"Page of the document, text";"Question category"
"3 of 20, Data description: The data description forms the basis for the data quality verification. A description and an exploration of the data is performed to gain insight about the underlying data generation process. The data should be described on a meta-level and by their statistical properties. Furthermore, a technically well funded visualization of the data should help to understand the data generating process [41 ]. Information about format, units and description of the input signals is expanded by domain knowledge.";"List every action, recommendation, best practice, guidelines, tip, norm and standard related to data and informations, how data have to be protected, classified, stored, and which mechanisms are implemented regarding data and data protection, and data validation."
"3 of 20, Data requirements: The data requirements can be defined either on the meta-level or directly in the data and should state the expected conditions of the data, i.e. whether a certain sample is plausible. The requirements can be, e.g., the expected feature values (a range for continuous features or a list for discrete features), the format of the data and the maximum number of missing values. The bounds of the requirements has to be defined carefully to include all possible real world values but discard non-plausible data. Data that does not satisfy the expected conditions could be treated as anomalies and have to be evaluated manually or excluded automatically. To mitigate the risk of anchoring bias in the definition phase discussing the requirements with a domain expert is advised [29 ]. Documentation of the data requirements could be done in the form of a schema [42,43].";"List every action, recommendation, best practice, guidelines, tip, norm and standard related to data and informations, how data have to be protected, classified, stored, and which mechanisms are implemented regarding data and data protection, and data validation."
"3 of 20, Data verification: The initial data, added data but also the production data has to be checked according to the requirements (see section 3.6). In cases where the requirements are not met, the data will be discarded and stored for further manual analysis. This helps to reduce the risk of decreasing the performance of the ML application through adding low-quality data and helps to detect varying data distributions or unstable inputs. To mitigate the risk of insufficient representation of extreme cases, it is best practice to use data exploration techniques to investigate the sample distribution.";"List every action, recommendation, best practice, guidelines, tip, norm and standard related to data and informations, how data have to be protected, classified, stored, and which mechanisms are implemented regarding data and data protection, and data validation."
"4 of 20, Data version control: Collecting data is not a static task but rather an iterative task. Modification on the data set (see section 3.2) should be documented to mitigate the risk of obtaining irreproducible or wrong results. Version control on the data is one of the essential tools to assure reproducibility and quality as it allows to track errors and unfavorable modifications during the development.";"List every action, recommendation, best practice, guidelines, tip, norm and standard related to secure design, secure development, secure deployment, secure operation, secure configuration, and secure maintenance."
"4 of 20, Feature selection: Selecting a good data representation based on the available measure-ments is one of the challenges to assure the quality of the ML application. It is best practice to discard underutilized features as they provide little to none modeling benefit but offer possible loopholes for errors i.e. instability of the feature during the operation of the ML application [ 30 ]. In addition, the more features are selected the more samples are necessary. Intuitively an exponentially increasing number of samples for an increasing number of features is required to prevent the data from becoming sparse in the feature space. This is termed as the curse of dimensionality [44,45 ]. Thus, it is best practice to select just necessary features. A checklist for the feature selection task is given in [46 ].";"List every action, recommendation, best practice, guidelines, tip, norm and standard related to secure design, secure development, secure deployment, secure operation, secure configuration, and secure maintenance."
"5 of 20, Data selection: Discarding samples should be well documented and strictly based on objective quality criteria. However, certain samples might not satisfy the necessary quality i.e. doesn't satisfy the requirements defined in section 3.1.5 and are not plausible and, thus, should be removed from the data set.";"List every action, recommendation, best practice, guidelines, tip, norm and standard related to secure design, secure development, secure deployment, secure operation, secure configuration, and secure maintenance."
"5 of 20, Unbalanced Classes: In cases of unbalanced classes, where the number of samples per class is skewed, different sampling strategies could improve the results. Over-sampling of the minority class and/or under-sampling of the majority class [ 56– 59 ] have been used. Over-sampling increases the importance of the minority class but could result in overfitting on the minority class. Under-Sampling by removing data points from the majority class has to be done carefully to keep the characteristics of the data and reduce the chance of introducing biases. However, removing points close to the decision boundary or multiple data points from the same cluster should be avoided. Comparing the results of different sampling techniques' reduces the risk of introducing bias to the model.";"List every action, recommendation, best practice, guidelines, tip, norm and standard related to secure design, secure development, secure deployment, secure operation, secure configuration, and secure maintenance."
"5 of 20, Noise reduction: The gathered data often includes, besides the predictive signal, noise and unwanted signals from other sources. Signal processing filters could be used to remove the irrelevant signals from the data and improve the signal-to-noise ratio [60 , 61]. However, filtering the data should be documented and evaluated because of the risk that an erroneous filter could remove important parts of the signal in the data.";"List every action, recommendation, best practice, guidelines, tip, norm and standard related to secure design, secure development, secure deployment, secure operation, secure configuration, and secure maintenance."
"5 of 20, Data imputation: To get a complete data set, missing, NAN and special values could be imputed with a model readable value. Depending on the data and ML task the values are imputed by mean or median values, interpolated, replaced by a special value symbol [ 62 ] (as the pattern of the values could be informative), substituted by model predictions [63 ], matrix factorization [ 64] or multiple imputations [ 65– 67] or imputed based on a convex optimization problem [ 68 ]. To reduce the risk of introducing substitution artifacts, the performance of the model should be compared between different imputation techniques.";"List every action, recommendation, best practice, guidelines, tip, norm and standard related to secure design, secure development, secure deployment, secure operation, secure configuration, and secure maintenance."
"5 of 20, Feature engineering: New features could be derived from existing ones based on domain knowledge. This could be, for example, the transformation of the features from the time domain into the frequency domain, discretization of continuous features into bins or augmenting the features with additional features based on the existing ones. In addition, there are several generic feature construction methods, such as clustering [ 69], dimensional reduction methods such as Kernel-PCA [ 70] or auto-encoders [71 ]. Nominal features and labels should be transformed into a one-hot encoding while ordinal features and labels are transformed into numerical values. However, the engineered features should be compared against a baseline to assess the utility of the feature. Underutilized features should be removed. Models that construct the feature representation as part of the learning process, e.g. neural networks, avoid the feature engineering steps [72].";"List every action, recommendation, best practice, guidelines, tip, norm and standard related to secure design, secure development, secure deployment, secure operation, secure configuration, and secure maintenance."
"6 of 20, Data augmentation: Data augmentation utilizes known invariances in the data to perform a label preserving transformation to construct new data. The transformations could either be performed in the feature space [57 ] or input space, such as applying rotation, elastic deformation or Gaussian noise to an image [73 ]. Data could also be augmented on a meta-level, such as switching the scenery from a sunny day to a rainy day. This expands the data set with additional samples and allows the model to capture those invariances.";"List every action, recommendation, best practice, guidelines, tip, norm and standard related to secure design, secure development, secure deployment, secure operation, secure configuration, and secure maintenance."
"6 of 20, File format: Some ML tools require specific variable or input types (data syntax). Indeed in practice, the comma separated values (CSV) format is the most generic standard (RFC 4180). ISO 8000 recommends the use of SI units according to the International System of Quantities. Defining a fix set of standards and units, helps to avoid the risks of errors in the merging process and further in detecting erroneous data (see section 3.1.5).";"List every action, recommendation, best practice, guidelines, tip, norm and standard related to secure design, secure development, secure deployment, secure operation, secure configuration, and secure maintenance."
"6 of 20, Normalization: Without proper normalization, the features could be defined on different scales and might lead to strong bias to features on larger scales. In addition, normalized features lead to faster convergence rates in neural networks than without [ 74 , 75]. Note that the normalization, applied to the training set has to be applied also to the test set using the same normalization parameters.";"List every action, recommendation, best practice, guidelines, tip, norm and standard related to secure design, secure development, secure deployment, secure operation, secure configuration, and secure maintenance."
"6 of 20, Literature research on similar problems: It is best practice to screen the literature (e.g. publications, patents, internal reports) for a comprehensive overview on similar ML tasks, since ML has become an established tool for a wide number of applications. New models can be based on published insights and previous results can serve as performance baselines.";"List every action, recommendation, best practice, guidelines, tip, norm and standard related to secure design, secure development, secure deployment, secure operation, secure configuration, and secure maintenance."
"6 of 20, Define quality measures of the model: The modeling strategy has to have multiple objectives in mind [76 ]. We suggest to evaluate the models on at least six complementary properties (see fig. 3). Besides a performance metric, soft measures such as robustness, explainability, scalability, resource demand and model complexity have to be evaluated (see table 2). The measures can be weighted differently depending on the application. In practical application, explainability [ 34 , 77, 78] or robustness might be valued more than accuracy. Additionally, the model's fairness [38,39] or trust might have to be assessed and mitigated.";"List every action, recommendation, best practice, guidelines, tip, norm and standard related to control, evaluation, verification, validation, and testing of systems, AI systems, networks, and components, also regarding their lifecycle."
"7 of 20, Model Selection: There are plenty of ML models and introductory books on classical methods [ 45 ,79 ] and Deep Learning [72 ] can be used to compare and understand their char-acteristics. The model selection depends on the data and has to be tailored to the problem. There is no such model that performs the best on all problem classes (No Free Lunch Theorem for ML [80 ]). It is best practice to start with models of lower capacity, which can serve as baseline, and gradually increase the capacity. Validating each step assures its benefit and avoid unnecessary complexity of the model.";"List every action, recommendation, best practice, guidelines, tip, norm and standard related to secure design, secure development, secure deployment, secure operation, secure configuration, and secure maintenance."
"7 of 20, Incorporate domain knowledge: In practice, a specialized model for a specific task performs better than a general model for all possible tasks. However, adapting the model to a specific problem involves the risk of incorporating false assumption and could reduce the solution space to a non-optimal subset. Therefore, it is best practice to validate the incorporated domain knowledge in isolation against a baseline. Adding domain knowledge should always increase the quality of the model, otherwise, it should be removed to avoid false bias.";"List every action, recommendation, best practice, guidelines, tip, norm and standard related to secure design, secure development, secure deployment, secure operation, secure configuration, and secure maintenance."
"7 of 20, Model training: The trained model depends on the learning problem and as such are tightly coupled. The learning problem contains an objective, optimizer, regularization and cross-validation [ 45, 72 ]. The objective of the learning problem depends on the application. Different applications value different aspects and have to be tweaked in alignment with the business success criteria. The objective is a proxy to evaluate the performance of the model. The optimizer defines the learning strategy and how to adapt the parameters of the model to improve the objective. Regularization which can be incorporated in the objective, optimizer and in the model itself is needed to reduce the risk of overfitting and can help to find unique solutions. Cross-validation is performed for feature selection, to optimize the hyper-parameters of the model and to test its generalization property to unseen data [ 81 ].";"List every action, recommendation, best practice, guidelines, tip, norm and standard related to secure design, secure development, secure deployment, secure operation, secure configuration, and secure maintenance."