Page of the document;text
28;Assessing the existing inherent risk level, and a second stage consisting in managing this risk by means of appropriate and proportional technical and organizational measures in order to remove or at least mitigate such risk, with the purposes of reducing the probability of impact of the identified threat. Once the chosen measures have been implemented, the remaining residual risk must be assessed and controlled.
28;In order to determine the risk and to establish the appropriate measures to manage it, the processing must be assessed and divided into stages. Therefore, the special requirements and risks of each stage must be managed from the data protection point of view.
28;In order to determine the level of risk of a processing, which is based on or includes stages that feature an AI component, it must be taking into account: • Any risks arising from the processing itself, in particular that derived from the bias present in decision-making systems and the natural persons discrimination (algorithmic discrimination). • Any risks arising for processing with regard to social context and collateral effects which may be derived from the processing, even those that are indirectly related to the purpose of the processing.
29;The PIA is an obligation laid down by the GDPR for high risk processing. This obligation requires to go further from the mere risk management related to the processing, and it requests an additional seriousness, accountability, at the time to manage such risk.
29;The need for each data controller to perform a data protection impact assessment is laid down in Article 35 of the GDPR when, as established in Paragraph 1, "the processing, is likely to result in a high risk to the rights and freedoms of natural persons".
29;More precisely, when the processing that is carried out in an automated way creates profiles and makes decisions, all such decisions must be identified in all phases of the processing, and the operating parameters must be analysed, such as, for example, error rates, and the effects that such decisions have on the data subjects must be carefully analysed.
30;The PIA must be documented and, when a high residual risk is detected, it must be subject to consultation before the supervisory authority under the conditions established in Article 36 of the GDPR.
29;The PIA is performed before the effective processing of the personal data, it means, before the actual operation of the processing. This means, inter alia, that the PIA must not be carried out during the processing validation phase, which includes the AI component, or simultaneously with the operation phase. Therefore, the validation must be performed before designing/selecting and implementing the AI solution for a processing so that the assessment of the privacy requirements can be made in advance and the privacy by design measures and the privacy by default measures may be effectively implemented.
30;A PIA must result in the adoption of a series of specific measures to manage risk, some of them aimed at reinforcing the obligations of fulfilment depending on such risk and affecting: • The conception of the processing itslef, its division in phases, procedures, technologies and extent of the processing. • The implementation of privacy by default measures and privacy by design measures in the processing pursuant to the principles of: o Minimising the amount of data that is being processed, both in terms of volume of information gathered and the size of the population that is the subject of the analysis, as well as throughout the different phases of the processing. o Aggregating the personal data to the extent possible, so as to reduce the level of detail that can be obtained as much as possible. o Concealing personal data and their interrelations to limit their exposure and to avoid their visibility before non-data subjects. o Separating the contexts of the processing to hinder the correlation of independent sources of information as well as the possibility to infer information.
30;o Proving, as the accountability principle states, that the data protection policy that is applicable is being fulfilled, as well as all other legal requirements and obligations imposed by the Regulation, with regard to the data subjects and the supervisory authorities. This includes a dynamic audit of the output/results of the processing, assessing the accuracy divergences and deviations, including the algorithms that have been executed, in order to adopt corrective measures. Among such measures are to cancel data and to document in detail of the analysis performed and the measures adopted. • The identification of the security requirements to reduce the risk with regard to privacy.
25;Therefore, the need to include the above measures to block any data related to the inference process (or at least inputs and results) which may be needed to respond a request or claim by the relevant data subject must be considered as a requirement when designing the processing. These mechanisms are also related to the log files that shall be discussed below.
23;Complying with this obligation by making a technical reference to the algorithm implementation may be obscure, confuse or excessive and leading to information fatigue. However, sufficient information must be provided to understand the behaviour of the relevant processing. Although it shall depend on the type of AI component used, an example of the type of information which may be relevant for the data subject would be: • Detailed information about the subject's data used for decision-making regardless of the category, especially regarding with how old the subject's data under processing are. • The relative importance or weight of each data category in the decision making. • The quality of training data and the type of patterns used. • Profiling activities conducted and their implications. • Error or precision values, according to the appropriate metrics used to measure the eligibility of the inference. • Whether qualified human supervision is involved or not. • Any reference to audits, especially on the possible deviation of inference results, as well as certification or certifications performed on the AI system. For adaptive systems or evolutive systems, the last audit conducted.
25;Article 32 of LOPDGDD establishes blocked data as the status of data kept outside the processing scope, applying any technical or organizational measures that prevent any type of process, including visualization, except for the purposes of making such data available to the relevant courts or judges, the public prosecutor or the competent Public Administration, especially the data protection authorities.
21;It must be taking into accout that to ground the lawfulness of a processing in a legitimate interest requires from the organization acting as a controller a higher degree of commitment, formality and competence. It requires a careful assessment that their legitimate interests prevail over the possible impact on the rights, freedoms and interests of the data subjects, considering, among other things, eventual compensatory measures arising from the need to keep the processing activities continuously supervised, implement a high degree of accountability and stricter design default privacy measures or the adoption of best practices such as giving an opt-out option to data subjects
27;As best practices, and beyond any requirement arising from data protection, human supervision may be an option which may be chosen within AI-based data processing, and in general with regard to automated decisions. The "dead man switch" approach must be avoided in system design: human users must have the option to ignore the algorithm at a given time in all cases, and to document the situations in which this course is privileged. For this reason, it is recommended to document any incidence or any challenges to automated decisions by the relevant data subject, so that its analysis allows to detect situations which require human intervention, because the processing is not operating as expected.
23;Any data controllers that use AI-based solutions to process personal data, conduct profiling or make automated decisions must be aware that the data subject have rights related to the protection of their personal data which must be attended to.
26;Any data controller who includes an AI component must assess and document whether their processing must provide portability, considering the provisions of the aforementioned article 20. In such case, the requirement of portability must be considered from the earlier stages of conceptualization and design, as well as in the selection of the AI component by the AI component developers.
27;When the legal grounds for processing is explicit consent, the controller must design the processing in such a way that it protects the free choice of users. This is done, first, by providing viable and equivalent alternatives to automated solutions at the time when their consent is required. Besides, it is guaranteed that if the data subject chooses not to be subject to automated decisions, the decision concerning such data subject shall not be biased and goes against the data subject's interests. If the above conditions are not met, consent may not be considered to have been freely given. These alternatives must be implemented from the processing design stage.