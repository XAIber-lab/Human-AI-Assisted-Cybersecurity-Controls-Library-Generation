Page number;Extract;Confidence Level
21;Executive Order 13960 on Promoting the Use of Trustworthy Artificial Intelligence requires that AI is: (a) lawful and respectful of our Nation's values; (b) purposeful and performance-driven; (c) accurate, reliable, and effective; (d) safe, secure, and resilient; (e) understandable; (f) responsible and traceable; (g) regularly monitored; (h) transparent; and, (i) accountable.;High
21;From large companies to start-ups, industry is providing innovative solutions that allow organizations to mitigate risks to the safety and efficacy of AI systems, both before deployment and through monitoring over time.;Medium, definition
21;These innovative solutions include risk assessments, auditing mechanisms, assessment of organizational procedures, dashboards to allow for ongoing monitoring, documentation procedures specific to model assessments, and many other strategies that aim to mitigate risks posed by the use of AI to companies' reputation, legal responsibilities, and other product safety and effectiveness concerns.;High
22;The National Institute of Standards and Technology (NIST) is developing a risk management framework to better manage risks posed to individuals, organizations, and society by AI.;Medium, definition
22;The NIST framework aims to foster the development of innovative approaches to address characteristics of trustworthiness including accuracy, explainability and interpretability, reliability, privacy, robustness, safety, security (resilience), and mitigation of unintended and/or harmful bias, as well as of harmful uses.;High
22;The NIST framework will consider and encompass principles such as transparency, accountability, and fairness during pre-design, design and development, deployment, use, and testing and evaluation of AI technologies and systems.;High
26;Any automated system should be tested to help ensure it is free from algorithmic discrimination before it can be sold or used.;High
26;Protection against algorithmic discrimination should include designing to ensure equity, broadly construed.;Medium
26;Those responsible for the development, use, or oversight of automated systems should conduct proactive equity assessments in the design phase of the technology research and development or during its acquisition to review potential input data, associated historical context, accessibility for people with disabilities, and societal goals to identify potential discrimination and effects on equity resulting from the introduction of the technology.;High
26;Any data used as part of system development or assessment should be representative of local communities based on the planned deployment setting and should be reviewed for bias based on the historical and societal context of the data.;High
27;Systems should be designed, developed, and deployed by organizations in ways that ensure accessibility to people with disabilities.;High
27;Automated systems should be tested using a broad set of measures to assess whether the system components, both in pre-deployment testing and in-context deployment, produce disparities.;High
27;Automated systems should be regularly monitored to assess algorithmic discrimination that might arise from unforeseen interactions of the system with inequities not accounted for during the pre-deployment testing, changes to the system after deployment, or changes to the context of use or associated data.;High
27;Monitoring and disparity assessment should be performed by the entity deploying or using the automated system to examine whether the system has led to algorithmic discrimination when deployed.;High
27;This assessment should be performed regularly and whenever a pattern of unusual results is occurring.;High
27;Riskier and higher-impact systems should be monitored and assessed more frequently.;High
28;Entities should allow independent evaluation of potential algorithmic discrimination caused by automated systems they use or oversee.;High
28;Entities responsible for the development or use of automated systems should provide reporting of an appropriately designed algorithmic impact assessment, with clear specification of who performs the assessment, who evaluates the system, and how corrective actions are taken (if necessary) in response to the assessment.;High
28;This algorithmic impact assessment should include at least: the results of any consultation, design stage equity assessments (potentially including qualitative analysis), accessibility designs and testing, disparity testing, document any remaining disparities, and detail any mitigation implementation and assessments.;High
28;Reporting should be provided in a clear and machine-readable manner using plain language to allow for more straightforward public accountability.;High
30;You should be protected from abusive data practices via built-in protections and you should have agency over how data about you is used.;High
30;You should be protected from violations of privacy through design choices that ensure such protections are included by default, including ensuring that data collection conforms to reasonable expectations and that only data strictly necessary for the specific context is collected.;High
30;Designers, developers, and deployers of automated systems should seek your permission and respect your decisions regarding collection, use, access, transfer, and deletion of your data in appropriate ways and to the greatest extent possible; where not possible, alternative privacy by design safeguards should be used.;High
30;Systems should not employ user experience and design decisions that obfuscate user choice or burden users with defaults that are privacy invasive.;High
30;Consent should only be used to justify collection of data in cases where it can be appropriately and meaningfully given.;High
30;Any consent requests should be brief, be understandable in plain language, and give you agency over data collection and the specific context of use.;High
30;Enhanced protections and restrictions for data and inferences related to sensitive domains, including health, work, education, criminal justice, and finance, and for data pertaining to youth should put you first.;High
30;You and your communities should be free from unchecked surveillance; surveillance technologies should be subject to heightened oversight that includes at least pre-deployment assessment of their potential harms and scope limits to protect privacy and civil liberties.;High
30;Continuous surveillance and monitoring should not be used in education, work, housing, or in other contexts where the use of such surveillance technologies is likely to limit rights, opportunities, or access.;High
30;Whenever possible, you should have access to reporting that confirms your data decisions have been respected and provides an assessment of the potential impact of surveillance technologies on your rights, opportunities, or access.;High