Page;Text;Confidence
10;"AI stakeholders can use ISO 27002 for the implementation and management of technical controls and also technical controls proposed by international organisations (e.g. SANS Top 20, UCI, CIS Critical Security Controls)";High
10;"AI stakeholders need to analyse potential attackers in order to estimate their risk levels more realistically and accurately and to undertake appropriate countermeasures";High
11;"Cybersecurity certification under the EU's Cybersecurity Act (CSA) is intended to increase trust and security for European consumers and businesses of ICT products (including the ones using AI technologies)";Medium
12;"The operators of ICT infrastructures need to be aware of and comply with all EU legislation, recommendations and directives, from the cybersecurity strategy in 2013 to the NIS 2 directive and the Cybersecurity Resilience Act in 2022";High
12;"NIS 2 and the CSA are considered to be Europe's two most important and far-reaching pieces of cybersecurity legislation and the general data protection regulation (GDPR) is the key personal data protection act, emphasising supply chain security and privacy respectively, which are most relevant for the life cycle of the AI systems as well";Medium, definition
13;"AI risk assessments should be dynamic and combined with anomaly detection approaches, as for ICT systems in general";High
13;"Measuring AI threats and evaluating AI risks require the development of a widely accepted scaling system that can meet common social and ethical values";High
13;"A taxonomy of AI attackers needs to advance the existing taxonomies, in order to better understand the motives, capabilities, objectives and psychological profiles of the AI adversaries";Medium
13;"Evaluation of an AI product against a static set of requirements can quickly become outdated, therefore dynamic RM and conformity assessment throughout the entire AI life cycle are required";High
14;"ensure that AI systems placed on the EU market or put into service are safe and respect existing law on fundamental rights and EU values";High
14;"ensure legal certainty to facilitate investment and innovation in AI";Medium
14;"enhance governance and effective enforcement of existing law on fundamental rights and safety requirements applicable to AI systems";High
14;"facilitate the development of a single market for lawful, safe and trustworthy AI applications and prevent market fragmentation";Medium
16;"AI assets: Data (Raw data, public data sets, training data, testing data, etc.), Models (Algorithms, models, model parameters, hyper-parameters, etc.), Artefacts (Data governance policies, descriptive statistical parameters, model frameworks, etc.), Actors/stakeholders (Data owners, data scientists, data engineers, model providers, etc.), Processes (Data ingestion, data pre-processing, data collection, data augmentation, feature selection, training, tuning, etc.), Environment/tools (Algorithm libraries, ML platforms, optimisation techniques, integrated development environments, etc.)";High, definition
17;"The additional required risk assessment efforts that are specific to AI must: include not only technical and physical threats, but also threats mentioned in the EU AI Act, such as loss of transparency, loss of interpretability, loss of managing bias and loss of accountability";High
18;"AI systems are socio-technical in nature, meaning that the threats are not only technical, legal or environmental (as in typical ICT systems), but social as well";Medium, definition
18;"social threats – such as bias, lack of fairness, lack of interpretability/explainability/equality – are directly connected to societal dynamics and human behaviour in all technical components of an AI system";High
19;"Accuracy: Correctness of output compared with reality; RM processes should consider the potential risks that might arise if the underlying causal relationship inferred by an AI model is not valid";High, definition
19;"Explainability: Provides a description of the conclusion/decision made in a way that can be understood by a human";High, definition
19;"Fairness: Neutrality of evidence, not biased by personal preferences, emotions or other limitations introduced by the context, equality (of gender and opportunity)";High, definition
19;"Privacy: Secure management (process, analysis, storage, transport, communication) of personal data and training models; ability to operate without disclosing information (data, model)";High, definition
19;"Reliability: Ability to maintain a minimum performance level and consistently generate the same results within the bounds of acceptable statistical errors";High, definition
19;"Resiliency: Ability to minimise impact, restore safe operating conditions and come out hardened from an adversarial attack";High, definition
19;"Robustness: Ability of an AI system to maintain a previously agreed minimum level of performance under any circumstances";High, definition
19;"Safety: Preventing unintended or harmful behaviour of the system to humans or society";High, definition
19;"Security: Ability to prevent deviations from safe operating conditions when undesirable events occur; ability to resist attacks; ensures confidentiality, integrity, authenticity, non-repudiation, availability of data, processes, services and models";High, definition
19;"Transparency: Ability to foster a general understanding of AI systems, make stakeholders aware of their interactions with AI systems and allow those affected by an AI system to understand the outcome. It also enables those adversely affected by an AI system to challenge its outcome based on plain and easy-to-understand information on the factors, and the logic that served as the basis for the prediction, recommendation or decision";High, definition