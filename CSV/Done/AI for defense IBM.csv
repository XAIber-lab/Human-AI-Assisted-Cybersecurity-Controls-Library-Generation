1;Detailing ethics practices throughout the AI lifecycle, corresponding to business (or mission) goals, data preparation and modeling, evaluation and deployment. The CRISP-DM model is useful here. IBM's Scaled Data Science Method, an extension of CRISP-DM, offers governance across the AI model lifecycle informed by collaborative input from data scientists, industrial-organizational psychologists, designers, communication specialists and others. The method merges best practices in data science, project management, design frameworks and AI governance. Teams can easily see and understand the requirements at each stage of the lifecycle, including documentation, who they need to talk to or collaborate with, and next steps.
2;Fostering an organizational culture that recognizes the sociotechnical nature of AI challenges. This must be communicated from the outset, and there must be a recognition of the practices, skill sets and thoughtfulness that need to be put into models and their management to monitor performance.
3;Providing interpretable AI model metadata (for example, as factsheets) specifying accountable persons, performance benchmarks (compared to human), data and methods used, audit records (date and by whom), and audit purpose and results.
3;Establishing a center of excellence to give diverse, multidisciplinary teams a community for applied training to identify potential disparate impact.
3;Using auditing tools to reflect the bias exhibited in models. If the reflection aligns with the values of the organization, transparency surrounding the chosen data and methods is key. If the reflection does not align with organizational values, then this is a signal that something must change. Discovering and mitigating potential disparate impact caused by bias involves far more than examining the data the model was trained on. Organizations must also examine people and processes involved. For example, have appropriate and inappropriate uses of the model been clearly communicated?
4;Always make clear to users when they are interfacing with an AI system.
4;Include human-in-the-loop as AI should augment and assist humans. This allows humans to provide feedback as AI systems operate.
4;Provide content grounding for AI models. Empower domain experts to curate and maintain trusted sources of data used to train models. Model output is based on the data it was trained on.
4;Capture key metadata to render AI models transparent and keep track of model inventory. Make sure that this metadata is interpretable and that the right information is exposed to the appropriate personnel. Data interpretation takes practice and is an interdisciplinary effort. At IBM, our Design for AI group aims to educate employees on the critical role of data in AI (among other fundamentals) and donates frameworks to the open-source community.
4;Make this metadata easily findable by people (ultimately at the source of output).
5;Using design thinking frameworks to assess unintended effects of AI models, determine the rights of the end users and operationalize principles. It's essential that design thinking exercises include people with widely varied lived experiencesâ€” the more diverse the better.
5;Create processes and frameworks to assess disparate impact and safety risks well before the model is deployed or procured. Designate accountable people to mitigate these risks.
5;Establishing communities that constantly reaffirm why fair, reliable outputs are essential. Many practitioners earnestly believe that simply by having the best intentions, there can be no disparate impact. This is misguided. Applied training by highly engaged community leaders who make people feel heard and included is critical.
5;Building reliability testing rationales around the guidelines and standards for data used in model training. The best way to make this real is to offer examples of what can happen when this scrutiny is lacking.
5;Limit user access to model development, but gather diverse perspectives at the onset of a project to mitigate introducing bias.
5;Perform privacy and security checks along the entire AI lifecycle.
5;Include measures of accuracy in regularly scheduled audits. Be unequivocally forthright about how model performance compares to a human being. If the model fails to provide an accurate result, detail who is accountable for that model and what recourse users have. (This should all be baked into the interpretable, findable metadata).
6;AI model investment does not stop at deployment. Dedicate resources to ensure models continue to behave as desired and expected. Assess and mitigate risk throughout the AI lifecycle, not just after deployment.
6;Designating an accountable party who has a funded mandate to do the work of governance. They must have power.
6;Invest in communication, community-building and education. Leverage tools such as watsonx.governance to monitor AI systems.
6;Capture and manage AI model inventory as described above.
6;Deploy cybersecurity measures across all models.
8;Measuring fairness and making equity standards actionable by providing functional and non-functional requirements for varying levels of service.