page;text;confidence_level
6;"generative technologies also introduce new risks and potential user harms. These risks include issues of copyright and intellectual property, the circumvention or reverse-engineering of prompts through attacks, the production of hateful, toxic, or profane language, the disclosure of sensitive or personal information, the production of malicious source code, and a lack of representation of minority groups due to underrepresentation in the training data";high
7;"Test & monitor for user harms. Identify relevant user harms (e.g. bias, toxic content, misinformation) and include mechanisms that test and monitor for them";high
7;"Expose or limit emergent behaviors. Determine whether generative capabilities beyond the intended use case should be surfaced to the user or restricted";medium
7;"Identify & resolve value tensions. Consider and balance different values across people involved in the creation, adoption, and usage of the AI system";medium
7;"Use a human-centered approach. Design for the user by understanding their needs and pain points, and not for the technology or its capabilities";low
7;"Teach effective use. Help the user learn how to effectively use the AI system by providing explanations of features and examples through in-context mechanisms and documentation";low
7;"Support co-editing of generated outputs. Allow both the user and the AI system to improve generated outputs";low
7;"Calibrate trust using explanations. Be clear and upfront about how well the AI system performs different tasks by explaining its capabilities and limitations";medium
7;"Use friction to avoid overreliance. Encourage the user to review and think critically about outputs by designing mechanisms that slow them down at key decision-making points";medium
7;"Make uncertainty visible. Caution the user that outputs may not align with their expectations and identify detectable uncertainties or flaws";medium
7;"Evaluate outputs using domain-specific metrics. Help the user identify outputs that satisfy measurable quality criteria";medium
7;"Provide feedback mechanisms. Collect user feedback to improve the training of the AI system";low
8;"Enable curation & annotation. Design user-driven or automated mechanisms for organizing, labeling, filtering, and/or sorting outputs";low
8;"Draw attention to differences or variations across outputs. Help the user identify how outputs generated from the same prompt differ from each other";low
8;"Understand the user's mental model. Build upon the user's existing mental models and evaluate how they think about your application: its capabilities, limitations, and how to work with it effectively";low
8;"Teach the AI system about the user. Capture the user's expectations, behaviors, and preferences to improve the AI system's interactions with them";low
Second set of extracts (second request):
page;text;confidence_level
5;"generative AI algorithms produce artifacts, rather than decision boundaries, as outputs";low
5;"one challenge of this paradigm stems from the distinguishing characteristic of generative AI: it generates artifacts as outputs and those outputs may vary in character or quality, even when a user's input does not change";medium
6;"alongside their tremendous potential to augment people's creative capabilities, generative technologies also introduce new risks and potential user harms. These risks include issues of copyright and intellectual property, the circumvention or reverse-engineering of prompts through attacks, the production of hateful, toxic, or profane language, the disclosure of sensitive or personal information, the production of malicious source code, and a lack of representation of minority groups due to underrepresentation in the training data";high
7;"Test & monitor for user harms. Identify relevant user harms (e.g. bias, toxic content, misinformation) and include mechanisms that test and monitor for them";high
7;"Expose or limit emergent behaviors. Determine whether generative capabilities beyond the intended use case should be surfaced to the user or restricted";high
7;"Evaluate outputs using domain-specific metrics. Help the user identify outputs that satisfy measurable quality criteria";high
7;"Make uncertainty visible. Caution the user that outputs may not align with their expectations and identify detectable uncertainties or flaws";high
7;"Use friction to avoid overreliance. Encourage the user to review and think critically about outputs by designing mechanisms that slow them down at key decision-making points";medium
7;"Calibrate trust using explanations. Be clear and upfront about how well the AI system performs different tasks by explaining its capabilities and limitations";medium
7;"Provide rationales for outputs. Show the user why a particular output was generated by identifying the source materials used to generate it";medium
7;"Signify the role of the AI. Determine the role the AI system will take within the user's workflow";low
8;"Understand the user's mental model. Build upon the user's existing mental models and evaluate how they think about your application: its capabilities, limitations, and how to work with it effectively";medium
8;"Teach the AI system about the user. Capture the user's expectations, behaviors, and preferences to improve the AI system's interactions with them";low