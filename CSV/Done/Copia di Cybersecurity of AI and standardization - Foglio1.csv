Text
"AI/ML components may be associated with hardware or other software components in order 
to mitigate the risk of functional failure, therefore changing the cybersecurity risks associated 
with the resulting set-up"
Reliable metrics can help a potential user detect a failure
"Testing procedures during the development process can lead to certain levels of 
accuracy/precision."
"High-risk AI systems which make use of 
techniques involving the training of models with 
data shall be developed on the basis of training, 
validation, and testing datasets that meet a set 
of quality criteria"
"High-risk AI systems shall be designed and 
developed with capabilities enabling the 
automatic recording of events (‘logs’) while the 
high-risk AI systems is operating. Those logging 
capabilities shall conform to recognised 
standards or common specifications.
"
"High-risk AI systems shall be designed and 
developed in such a way to ensure that their 
operation is sufficiently transparent to enable 
users to interpret the system’s output and use it 
appropriately. An appropriate type and degree of 
transparency shall be ensured, with a view to 
achieving compliance with the relevant 
obligations of the user and of the provider set 
out in Chapter 3 of [COM(2021) 206 final]."
"High-risk AI systems shall be designed and 
developed in such a way, including with 
appropriate human–machine interface tools, that 
they can be effectively overseen by natural 
persons during the period in which the AI system 
is in use.
"
"An assessment through internal checks for 
‘stand-alone’ high-risk AI systems would require 
a full, effective and properly documented ex ante 
compliance with all requirements of the 
regulation and compliance with robust quality 
and risk management systems and post-market 
monitoring. 
A risk management system shall be established, 
implemented, documented and maintained in 
relation to high-risk AI systems.
"
"Providers of high-risk AI systems shall put a 
quality management system in place that 
ensures compliance with this Regulation. 
The provider should establish a sound quality 
management system, ensure the 
accomplishment of the required conformity 
assessment procedure, draw up the relevant 
documentation and establish a robust post market monitoring system."
"AI systems that create a high risk to the health 
and safety or fundamental rights of natural 
persons: in line with a risk-based approach, 
these high-risk AI systems are permitted on the 
European market subject to compliance with 
certain mandatory requirements and an ex-ante 
conformity assessment."
"AI systems should be resilient against risks 
connected to the limitations of the system (e.g. 
errors, faults, inconsistencies, unexpected 
situations) as well as against malicious actions 
that may compromise the security of the AI 
system and result in harmful or otherwise 
undesirable behaviour.
"
"Given the applicability of AI in a wide range of domains, the identification of
cybersecurity risks and the determination of appropriate security requirements should 
rely on a system-specific analysis and, where needed, on sectorial standards."
"It is important to develop the guidance necessary to back up existing technical and 
organisational standards that can support the cybersecurity of AI systems, while 
monitoring R&D advancements"
"Ensure that the actors performing conformity assessment on AI systems have 
standardised tools and competences, including on cybersecurity"
Ensure regulatory coherence between the draft AI Act and legislation on cybersecurity.