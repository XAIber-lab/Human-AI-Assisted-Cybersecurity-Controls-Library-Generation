page_text_First proposal_Second proposal_Third proposal_labels_confidence_definitions
2_adopting a data protection by design approach; conducting a DPIA; and ensuring appropriate governance measures around your deployment act as safeguards against unfairness_GV.PO-01; GV.PO-02_ID.RA-05_GV.RR-01_privacy_High_
2_If you process personal data, you must consider the data protection principles across the AI lifecycle, not just at specific stages. For example, when you use AI to make solely automated decisions, you must consider an individual's ability to understand and contest those decisions, throughout the lifecycle_GV.OC-03_PR.AT-01_GV.PO-01_privacy; transparency_High_
2_assess whether it works as intended; how it interacts or influences your entire decision-making process; whether it reinforces or replicates unfair discrimination; and any detrimental unintended consequences that may arise_ID.RA-04; ID.RA-05_DE.AE-04_ID.IM-01__High_
3_The specific technical and organisational measures you need to adopt depend on the nature, scope, context, and purpose of your processing and the particular risks it poses. In any case, you should test the effectiveness of these measures at the design stage and detail: how the measures mitigate risks; and whether they introduce other risks, and steps you will take to manage these_GV.RM-06_ID.RA-05_GV.OV-03__High_
3_information around the demographic groups a model was originally or continues to be trained on; what, if any, underlying bias has been detected or could emerge; or any algorithmic fairness testing that has already been conducted_ID.RA-01; ID.RA-03_DE.AE-02_GV.SC-07_transparency_Medium_
6_You could adopt a participatory design approach Independent domain expertise and lived experience testimony will help you identify and address fairness risks. This includes relative disadvantage and real-world societal biases that may otherwise appear in your datasets and consequently your AI outputs over time_GV.RR-02_PR.AT-02_ID.RA-03__High_
6_For engagement with impacted groups to be meaningful, they or their representatives need to know about: the possibilities and limitations of your AI system; and the risks inherent in the dynamic nature of the environments you deploy it in_GV.OC-02_RS.CO-02_PR.AT-01_transparency_High_
7_Your organisation as a whole may not have the necessary knowledge of what marginalisation means in all possible contexts in which you may use your AI system. It is important that key decision-makers at management level acknowledge this. Seeking and incorporating advice from independent domain experts is good practice_GV.RR-01; GV.RR-02_PR.AT-02___Medium_definition
7_You should determine and document your approach to bias and discrimination mitigation from the very beginning of any AI application lifecycle. You should take into account and put in place the appropriate safeguards, technical and organisational measures during the design and build phase_GV.PO-01_ID.RA-05_GV.RM-01__High_
9_When you use AI to process personal data, you take decisions about what data to include and why. This applies whether you collect the data from individuals directly, or whether you obtain it from elsewhere_ID.AM-07_GV.OC-03_PR.DS-01_data protection_Medium_definition
9_The purpose limitation and data minimisation principles mean you have to be clear about the personal data you need to achieve your purpose, and only collect that data. Your data should be adequate for your intended purpose_PR.DS-01_GV.PO-01_ID.AM-07_data protection_High_
9_To ensure your processing and outcomes are fair, your datasets should be accurate, complete and representative of the purpose of the processing_PR.DS-01; PR.DS-02_ID.AM-07_GV.OC-01_data protection_High_
14_Training your staff about implicit bias and how it may impact their decisions is one way of mitigating these risks. Including community groups and impacted individuals in the labelling process is another, and is an example of participatory design in practice_PR.AT-01; PR.AT-02_GV.RR-04___High_
14_In general, your training data labels should comply with data protection's accuracy principle. You should clearly tag output data as inferences and predictions, and not claim it to be factual. You should document clear criteria and lines of accountability for labelling data_PR.DS-01_ID.AM-07_GV.PO-01_data protection_High_
15_If, depending on your context, you need to use bias mitigation techniques using algorithmic fairness metrics, you should track the provenance, development, and use of your training datasets_ID.AM-07; ID.AM-08_PR.DS-01_GV.OC-03__High_
15_To ensure fairness, you may decide not to use certain data sources or features to make decisions about people. For example, when using them may lead to direct or indirect discrimination. In these cases, you should assess, document and record the sources or features that you do not intend to use_ID.RA-05_PR.DS-01_GV.PO-01__High_
15_The technical approaches we set out in this annex are neither exhaustive nor a 'silver bullet'. Determining which are appropriate will be a process of trial and error, particularly as techniques evolve and mature, or demonstrate limitations as they do so_ID.IM-01_GV.OC-02___Low_definition
20_You should monitor the statistical accuracy of 'hybrid' AI systems that involve human and AI cooperation. You should use the outcomes of this monitoring to improve your system or decide to withdraw it, if unmitigable high risks arise after deployment_DE.CM-03_ID.IM-01_RS.MA-04__High_
22_You should consider safeguards for how you apply your AI system to subgroups of the population. If you embed algorithmic fairness constraints into your model or test it post-development, you should examine whether your safeguards protect individuals as well as groups_ID.RA-05_GV.PO-01_PR.DS-01__High_
22_If your system processes personal data fairly about most people but unfairly for one individual, your processing will still infringe the fairness principle. Identifying what makes individuals vulnerable will also assist you in determining the appropriate safeguards_ID.RA-01; ID.RA-03_GV.OC-02_PR.DS-01_privacy_High_
22_You need to pay particular attention to 'edge cases' or outliers. For example, cases where your ML model makes incorrect predictions or classifications because you have not trained it using sufficient data about similar individuals_ID.RA-01_DE.AE-02_PR.DS-01__High_
23_You could conduct disaggregated evaluations and keep a record of disparities in outcomes detected. This is not just to mitigate them, but to assist your senior management in understanding and addressing these issues appropriately and promptly_DE.AE-02; DE.AE-03_ID.IM-01_GV.OV-03__High_
23_If you procure AI models, you should seek assurances from the AI vendors about any bias testing they conducted on them or test the models yourself_GV.SC-06_ID.RA-09___High_
26_After deployment you should monitor your AI system's performance across groups. The frequency of the monitoring should be proportional to the impact of incorrect outputs on individuals and vulnerable groups_DE.CM-03_ID.RA-05_GV.OV-03__High_
26_You must also have a transparent and simple-to-use mechanism that allows impacted individuals and groups to contest decisions they deem unfair_GV.OC-02_RS.CO-02_PR.AA-05_transparency_High_
26_AI systems often enable you to process personal data on a large scale and at speed. Therefore, your processes to address the risk of unfair outcomes should be designed so that you can deal with these issues in a timely manner_ID.RA-05_RS.MA-04_DE.CM-03_data protection_Medium_definition
29_You should consider how you will deprecate the functionality and the backend infrastructure of your system, along with any personal data processed, if necessary. For example, you may need to decommission or replace the system, if it: ends up not meeting the benchmarks for its use; stops fulfilling its initial purpose; or has resulted in material or non-material harms_ID.AM-08_PR.PS-02; PR.PS-03_GV.OV-03__High_
29_In the case of mission-critical systems that cannot be allowed to fail, you need to have a transition or replacement plan in place. For example, systems processing interbank payments. You should plan for this at the design stage or as part of the development roadmaps_GV.PO-01; GV.PO-02_ID.IM-04_PR.IR-03__High_
29_You should consider how you: incorporate decommissioning, including code, datasets, liabilities, risks, and responsibilities into your roadmap; take steps to erase or anonymise any personal data once or if you decide to withdraw the AI system; and ensure your decommissioning process is independently verifiable and auditable_ID.AM-08_PR.PS-02; PR.PS-03_GV.OC-03_data protection_High_