text_extract;highlight_color
"Improved machine perception that for certain tasks can exceed human cognitive visual performance, Optimization and planning engines that leverage reinforcement learning to exceed human performance in complex games, Generative algorithms that can create text, audio, and images that in many cases are indistinguishable from those created by humans without targeted analysis";red
"Tools are emerging to identify and protect against new and existing threats. For example, MITRE works closely with industry and government to capture such threats and document associated adversary tactics, techniques, and procedures in the MITRE Adversarial Threat Landscape for Artificial-Intelligence Systems (ATLAS) framework";yellow
"MITRE defines AI assurance as a lifecycle process that provides justified confidence in an AI system's ability to operate effectively with acceptable levels of risk to its stakeholders";green
"The risks that need to be managed within acceptable levels may be associated with or stem from a variety of factors depending on the use context, including but not limited to AI system safety, security, equity, reliability, interpretability, robustness, privacy, and governability";green
"The National Institute of Standards and Technology's (NIST) AI Risk Management Framework (RMF) is a good example of an approach that incorporates trustworthiness considerations into AI design, development, use, and evaluation of AI system components";yellow
"Industry regulators should promote trusted information sharing mechanisms to support regulatory analysis";green
"Any AI regulation should require AI components to satisfy software assurance requirements as well as AI-specific assurance requirements that can be developed based on validated AI assurance frameworks";green
"Regulated industries should develop a NIST AI RMF response plan; if they deem the NIST AI RMF insufficient, they should identify alternative AI assurance approaches";green
"Any AI regulation should account for and mitigate risks stemming from component interactions";green
"Any AI regulation should account for use context and favor existing domain-specific regulations";green
"Any AI regulation should require 'assurance cases' to be developed before deployment";green
"An assurance case is a documented body of evidence that provides a compelling argument that the system satisfies certain critical assurance properties in specific contexts";yellow
"Industry regulators should conduct continuous regulatory analysis of individual use cases";green
"AI regulation should require system auditability in order to hold individuals who misuse AI to cause harm accountable";green
"Legal frameworks that may be developed as part of AI regulation to hold individuals accountable for causing harm with AI should focus on regulatory approaches commensurate with the scale of the risk";green
"AI regulation should provide appropriate levels of transparency into AI applications to an objective third party and/or the public for detection and mitigation of intentional AI misuse";green
"We recommend an assessment of federal government critical infrastructure plans focused on identifying and strengthening recommendations for safety-critical cyber-physical systems particularly vulnerable to increased threats due to the scale and speed AI enables";yellow
"Federal government critical infrastructure plans should address increased risk due to AI-enabled scale and speed and consider countering risk with automated red teaming";green
"Increase federal funding to create common vocabulary and frameworks for AI alignment, and use those to guide future research";yellow
"Regulation and legal frameworks should differentiate between appropriate research with risk mitigations and bad actors, and hold all appropriately accountable for harms";green
"Building on ATLAS and in partnership with Microsoft, MITRE released tools to perform red team testing of converged AI-cyber systems as Arsenal";red