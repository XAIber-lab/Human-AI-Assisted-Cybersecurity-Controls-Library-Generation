page_text_First proposal_Second proposal_Third proposal_labels_confidence_definition
EN 44;the prevention of a specific, substantial and imminent threat to the life or physical safety of natural persons or of a terrorist attack_ID.RA-03_ID.RA-05_RS.MI-01__Medium_
EN 44;take into account the following elements: (a) the nature of the situation giving rise to the possible use, in particular the seriousness, probability and scale of the harm caused in the absence of the use of the system_ID.RA-04_ID.RA-05_GV.RM-06__High_
EN 44;take into account the consequences of the use of the system for the rights and freedoms of all persons concerned, in particular the seriousness, probability and scale of those consequences_ID.RA-04_ID.RA-05_GV.OC-03_privacy_High_
EN 44;shall comply with necessary and proportionate safeguards and conditions in relation to the use, in particular as regards the temporal, geographic and personal limitations_PR.AA-05_GV.PO-01_ID.RA-07__High_
EN 44;be subject to a prior authorisation granted by a judicial authority or by an independent administrative authority of the Member State_GV.OC-03_GV.PO-01___High_
EN 46;A risk management system shall be established, implemented, documented and maintained in relation to high-risk AI systems_GV.RM-01_GV.RM-03___High_
EN 46;The risk management system shall consist of a continuous iterative process run throughout the entire lifecycle of a high-risk AI system, requiring regular systematic updating_GV.RM-02_ID.RA-07___High_definition
EN 47;identification and analysis of the known and foreseeable risks associated with each high-risk AI system_ID.RA-01; ID.RA-03_ID.RA-04___High_
EN 47;estimation and evaluation of the risks that may emerge when the high-risk AI system is used in accordance with its intended purpose and under conditions of reasonably foreseeable misuse_ID.RA-04_ID.RA-05___High_
EN 47;evaluation of other possibly arising risks based on the analysis of data gathered from the post-market monitoring system_ID.RA-05_DE.AE-02___High_
EN 47;adoption of suitable risk management measures_GV.RM-04_ID.RA-06___High_
EN 47;elimination or reduction of risks as far as possible through adequate design and development_ID.RA-06_PR.PS-06___High_
EN 47;implementation of adequate mitigation and control measures in relation to risks that cannot be eliminated_RS.MI-01_RS.MI-02___High_
EN 47;provision of adequate information pursuant to Article 13, in particular as regards the risks_RS.CO-02_RS.CO-03_GV.RM-05_transparency_High_
EN 47;due consideration shall be given to the technical knowledge, experience, education, training to be expected by the user and the environment in which the system is intended to be used_PR.AT-01; PR.AT-02_GV.OC-02___High_
EN 47;High-risk AI systems shall be tested for the purposes of identifying the most appropriate risk management measures_PR.PS-06_ID.IM-02___High_
EN 47;Testing shall ensure that high-risk AI systems perform consistently for their intended purpose and they are in compliance with the requirements_PR.PS-06_ID.IM-02_GV.OC-03__High_
EN 47;Testing procedures shall be suitable to achieve the intended purpose of the AI system and do not need to go beyond what is necessary to achieve that purpose_PR.PS-06_ID.IM-02___Medium_
EN 47;testing of the high-risk AI systems shall be performed, as appropriate, at any point in time throughout the development process, and, in any event, prior to the placing on the market or the putting into service_PR.PS-06_ID.IM-02_ID.AM-08__High_
EN 47;Testing shall be made against preliminarily defined metrics and probabilistic thresholds that are appropriate to the intended purpose of the high-risk AI system_PR.PS-06_ID.IM-02_GV.RM-06__High_
EN 48;specific consideration shall be given to whether the high-risk AI system is likely to be accessed by or have an impact on children_ID.RA-04_GV.OC-02___High_
EN 48;High-risk AI systems which make use of techniques involving the training of models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria_PR.DS-01_ID.AM-07_PR.PS-06_data protection_High_
EN 48;data governance and management practices shall concern in particular: (a) the relevant design choices; (b) data collection; (c) relevant data preparation processing operations, such as annotation, labelling, cleaning, enrichment and aggregation_PR.DS-01_ID.AM-07_GV.PO-01_data protection_High_
EN 48;the formulation of relevant assumptions, notably with respect to the information that the data are supposed to measure and represent_ID.RA-04_PR.DS-01___Medium_
EN 48;a prior assessment of the availability, quantity and suitability of the data sets that are needed_PR.DS-01_ID.AM-07___High_
EN 48;examination in view of possible biases_ID.RA-01_PR.DS-01_PR.PS-06__High_
EN 48;the identification of any possible data gaps or shortcomings, and how those gaps and shortcomings can be addressed_ID.RA-01_PR.DS-01_ID.IM-01__High_
EN 48;Training, validation and testing data sets shall be relevant, representative, free of errors and complete_PR.DS-01_ID.AM-07_PR.PS-06_data protection_High_
EN 48;Training, validation and testing data sets shall take into account, to the extent required by the intended purpose, the characteristics or elements that are particular to the specific geographical, behavioural or functional setting_PR.DS-01_ID.AM-07_GV.OC-02_data protection_High_
EN 48;process special categories of personal data subject to appropriate safeguards for the fundamental rights and freedoms of natural persons, including technical limitations on the re-use and use of state-of-the-art security and privacy-preserving measures, such as pseudonymisation, or encryption where anonymisation may significantly affect the purpose pursued_PR.DS-01; PR.DS-02_PR.AA-05_GV.OC-03_privacy; data protection_High_
EN 49;The technical documentation shall be drawn up before that system is placed on the market or put into service and shall be kept up-to date_ID.AM-08_GV.PO-01___High_
EN 49;High-risk AI systems shall be designed and developed in such a way to ensure that their operation is sufficiently transparent to enable users to interpret the system's output and use it appropriately_PR.PS-06_GV.OC-02_GV.PO-01_transparency_High_
EN 49;High-risk AI systems shall be accompanied by instructions for use in an appropriate digital format or otherwise that include concise, complete, correct and clear information that is relevant, accessible and comprehensible to users_GV.OC-02_PR.AT-01_RS.CO-02_transparency_High_
EN 49;the identity and the contact details of the provider and, where applicable, of its authorised representative_GV.RR-02_GV.SC-02___Medium_
EN 49;the level of accuracy, robustness and cybersecurity referred to in Article 15 against which the high-risk AI system has been tested and validated and which can be expected_PR.PS-06_ID.RA-09_DE.CM-09_transparency_High_
EN 49;any known or foreseeable circumstance, related to the use of the high-risk AI system in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, which may lead to risks to the health and safety or fundamental rights_ID.RA-03_ID.RA-04_RS.CO-02_transparency_High_
EN 49;its performance as regards the persons or groups of persons on which the system is intended to be used_ID.RA-04_GV.OC-02___Medium_
EN 49;specifications for the input data, or any other relevant information in terms of the training, validation and testing data sets used, taking into account the intended purpose of the AI system_PR.DS-01_ID.AM-07_PR.PS-06_data protection; transparency_High_
EN 50;High-risk AI systems shall be designed and developed in such a way, including with appropriate human-machine interface tools, that they can be effectively overseen by natural persons during the period in which the AI system is in use_PR.PS-06_GV.RR-02_PR.AT-01_transparency_High_
EN 50;Human oversight shall aim at preventing or minimising the risks to health, safety or fundamental rights that may emerge when a high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse_GV.RR-02_ID.RA-05_PR.AT-01__High_
EN 50;fully understand the capacities and limitations of the high-risk AI system and be able to duly monitor its operation, so that signs of anomalies, dysfunctions and unexpected performance can be detected and addressed as soon as possible_PR.AT-01; PR.AT-02_DE.CM-09_DE.AE-02__High_
EN 50;remain aware of the possible tendency of automatically relying or over-relying on the output produced by a high-risk AI system ('automation bias')PR.AT-01_GV.RR-01___High
EN 50;be able to correctly interpret the high-risk AI system's output, taking into account in particular the characteristics of the system and the interpretation tools and methods available_PR.AT-01_PR.AT-02_GV.OC-02_transparency_High_
EN 50;be able to decide, in any particular situation, not to use the high-risk AI system or otherwise disregard, override or reverse the output of the high-risk AI system_GV.RR-02_PR.AT-01___High_
EN 50;be able to intervene on the operation of the high-risk AI system or interrupt the system through a "stop" button or a similar procedure_PR.IR-03_RS.MI-01___High_
EN 50;no action or decision is taken by the user on the basis of the identification resulting from the system unless this has been verified and confirmed by at least two natural persons_GV.RR-02_PR.AA-05___High_
EN 51;High-risk AI systems shall be designed and developed in such a way that they achieve, in the light of their intended purpose, an appropriate level of accuracy, robustness and cybersecurity, and perform consistently in those respects throughout their lifecycle_PR.PS-06_PR.IR-03_ID.AM-08__High_
EN 51;The levels of accuracy and the relevant accuracy metrics of high-risk AI systems shall be declared in the accompanying instructions of use_PR.PS-06_GV.OC-02_RS.CO-02_transparency_High_
EN 52;High-risk AI systems shall be resilient as regards errors, faults or inconsistencies that may occur within the system or the environment in which the system operates, in particular due to their interaction with natural persons or other systems_PR.IR-03_PR.PS-06_DE.CM-09__High_
EN 52;The robustness of high-risk AI systems may be achieved through technical redundancy solutions, which may include backup or fail-safe plans_PR.IR-03_PR.DS-11___Medium_
EN 52;High-risk AI systems that continue to learn after being placed on the market or put into service shall be developed in such a way to ensure that possibly biased outputs due to outputs used as an input for future operations ('feedback loops') are duly addressed with appropriate mitigation measures_PR.PS-06_ID.RA-07_RS.MI-01__High_
EN 52;High-risk AI systems shall be resilient as regards attempts by unauthorised third parties to alter their use or performance by exploiting the system vulnerabilities_PR.IR-03_PR.PS-01_ID.RA-01__High_
EN 52;The technical solutions aimed at ensuring the cybersecurity of high-risk AI systems shall be appropriate to the relevant circumstances and the risks_PR.IR-03_ID.RA-05___High_
EN 52;The technical solutions to address AI specific vulnerabilities shall include, where appropriate, measures to prevent and control for attacks trying to manipulate the training dataset ('data poisoning'), inputs designed to cause the model to make a mistake ('adversarial examples'), or model flaws_PR.PS-06_ID.RA-01_RS.MI-01_data protection_High_
EN 53;draw-up the technical documentation of the high-risk AI system_ID.AM-08_GV.PO-01___High_
EN 53;when under their control, keep the logs automatically generated by their high-risk AI systems_PR.PS-04_DE.CM-09___High_
EN 53;ensure that the high-risk AI system undergoes the relevant conformity assessment procedure, prior to its placing on the market or putting into service_ID.RA-09_GV.PO-01___High_
EN 53;take the necessary corrective actions, if the high-risk AI system is not in conformity with the requirements_RS.MI-01_ID.IM-01___High_
EN 53;inform the national competent authorities of the Member States in which they made the AI system available or put it into service and, where applicable, the notified body of the non-compliance and of any corrective actions taken_RS.CO-02_GV.OC-03___High_
EN 53;affix the CE marking to their high-risk AI systems to indicate the conformity with this Regulation_GV.OC-03_ID.AM-08___Medium_
EN 53;upon request of a national competent authority, demonstrate the conformity of the high-risk AI system with the requirements_GV.OC-03_ID.AM-08___High_
EN 54;a strategy for regulatory compliance, including compliance with conformity assessment procedures and procedures for the management of modifications to the high-risk AI system_GV.PO-01_GV.OC-03___High_
EN 54;techniques, procedures and systematic actions to be used for the design, design control and design verification of the high-risk AI system_PR.PS-06_ID.IM-02___High_
EN 54;techniques, procedures and systematic actions to be used for the development, quality control and quality assurance of the high-risk AI system_PR.PS-06_ID.IM-02___High_
EN 54;examination, test and validation procedures to be carried out before, during and after the development of the high-risk AI system, and the frequency with which they have to be carried out_PR.PS-06_ID.IM-02___High_
EN 54;technical specifications, including standards, to be applied and, where the relevant harmonised standards are not applied in full, the means to be used to ensure that the high-risk AI system complies with the requirements_PR.PS-06_GV.PO-01___High_
EN 54;systems and procedures for data management, including data collection, data analysis, data labelling, data storage, data filtration, data mining, data aggregation, data retention and any other operation regarding the data that is performed before and for the purposes of the placing on the market or putting into service of high-risk AI systems_PR.DS-01_ID.AM-07_PR.PS-06_data protection_High_
EN 54;the risk management system referred to in Article 9_GV.RM-01_GV.RM-03___High_
EN 54;the setting-up, implementation and maintenance of a post-market monitoring system_DE.CM-09_ID.IM-03___High_
EN 54;procedures related to the reporting of serious incidents and of malfunctioning_RS.CO-02_RS.CO-03___High_
EN 54;the handling of communication with national competent authorities, competent authorities, including sectoral ones, providing or supporting the access to data, notified bodies, other operators, customers or other interested parties_RS.CO-02_GV.SC-02___High_
EN 54;systems and procedures for record keeping of all relevant documentation and information_ID.AM-08_PR.PS-04___High_
EN 54;resource management, including security of supply related measures_GV.SC-01_GV.RR-03___High_
EN 54;an accountability framework setting out the responsibilities of the management and other staff with regard to all aspects listed in this paragraph_GV.RR-01_GV.RR-02__transparency_High_
EN 55;Providers shall keep the logs automatically generated by their high-risk AI systems, to the extent such logs are under their control by virtue of a contractual arrangement with the user or otherwise by law_PR.PS-04_DE.CM-09___High_
EN 55;The logs shall be kept for a period that is appropriate in the light of the intended purpose of high-risk AI system and applicable legal obligations under Union or national law_PR.PS-04_GV.OC-03___High_
EN 55;High-risk AI systems shall undergo a new conformity assessment procedure whenever they are substantially modified, regardless of whether the modified system is intended to be further distributed or continues to be used by the current user_ID.RA-07_GV.PO-02___High_
EN 56;Providers of high-risk AI systems shall, upon request by a national competent authority, provide that authority with all the information and documentation necessary to demonstrate the conformity of the high-risk AI system with the requirements_GV.OC-03_ID.AM-08___High_
EN 56;Upon a reasoned request from a national competent authority, providers shall also give that authority access to the logs automatically generated by the high-risk AI system_PR.PS-04_GV.OC-03___High_
EN 56;cooperate with competent national authorities, upon a reasoned request, on any action the latter takes in relation to the high-risk AI system_GV.OC-03_RS.CO-02___High_
EN 56;Before placing a high-risk AI system on the market, importers of such system shall ensure that: (a) the appropriate conformity assessment procedure has been carried out by the provider of that AI system_GV.SC-06_ID.RA-09___High_
EN 56;the provider has drawn up the technical documentation in accordance with Annex IV_ID.AM-08_GV.SC-06___High_
EN 56;the system bears the required conformity marking and is accompanied by the required documentation and instructions of use_GV.OC-03_GV.SC-06___High_
EN 56;Where an importer considers or has reason to consider that a high-risk AI system is not in conformity with this Regulation, it shall not place that system on the market until that AI system has been brought into conformity_GV.SC-06_ID.RA-09___High_
EN 56;Where the high-risk AI system presents a risk within the meaning of Article 65(1), the importer shall inform the provider of the AI system and the market surveillance authorities to that effect_RS.CO-02_GV.SC-02___High_
EN 56;Importers shall indicate their name, registered trade name or registered trade mark, and the address at which they can be contacted on the high-risk AI system or, where that is not possible, on its packaging or its accompanying documentation_GV.SC-02_ID.AM-08___Medium_
EN 56;Importers shall ensure that, while a high-risk AI system is under their responsibility, where applicable, storage or transport conditions do not jeopardise its compliance with the requirements_GV.SC-07_PR.IR-02___High_
EN 57;Before making a high-risk AI system available on the market, distributors shall verify that the high-risk AI system bears the required CE conformity marking, that it is accompanied by the required documentation and instruction of use_GV.SC-06_ID.RA-09___High_
EN 57;Where a distributor considers or has reason to consider that a high-risk AI system is not in conformity with the requirements set out in Chapter 2 of this Title, it shall not make the high-risk AI system available on the market until that system has been brought into conformity with those requirements_GV.SC-06_ID.RA-09___High_
EN 57;Distributors shall ensure that, while a high-risk AI system is under their responsibility, where applicable, storage or transport conditions do not jeopardise the compliance of the system with the requirements_GV.SC-07_PR.IR-02___High_
EN 57;A distributor that considers or has reason to consider that a high-risk AI system which it has made available on the market is not in conformity with the requirements set out in Chapter 2 of this Title shall take the corrective actions necessary to bring that system into conformity with those requirements, to withdraw it or recall it_RS.MI-01_GV.SC-07___High_
EN 58;Users of high-risk AI systems shall use such systems in accordance with the instructions of use accompanying the systems_PR.AT-01_GV.OC-02___High_
EN 58;to the extent the user exercises control over the input data, that user shall ensure that input data is relevant in view of the intended purpose of the high-risk AI system_PR.DS-01_PR.AT-01_DE.CM-09_data protection_High_
EN 58;Users shall monitor the operation of the high-risk AI system on the basis of the instructions of use_DE.CM-09_PR.AT-01___High_
EN 58;When they have reasons to consider that the use in accordance with the instructions of use may result in the AI system presenting a risk within the meaning of Article 65(1) they shall inform the provider or distributor and suspend the use of the system_RS.CO-02_RS.MI-01___High_
EN 58;They shall also inform the provider or distributor when they have identified any serious incident or any malfunctioning within the meaning of Article 62 and interrupt the use of the AI system_RS.CO-02_RS.MI-01___High_
EN 58;Users of high-risk AI systems shall keep the logs automatically generated by that high-risk AI system, to the extent such logs are under their control_PR.PS-04_DE.CM-09___High_
EN 58;The logs shall be kept for a period that is appropriate in the light of the intended purpose of the high-risk AI system and applicable legal obligations under Union or national law_PR.PS-04_GV.OC-03___High_