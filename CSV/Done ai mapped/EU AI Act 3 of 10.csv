1_It is appropriate to establish a methodology for the classification of general-purpose AI models as general-purpose AI model with systemic risks. Since systemic risks result from particularly high capabilities, a general-purpose AI model should be considered to present systemic risks if it has high-impact capabilities, evaluated on the basis of appropriate technical tools and methodologies, or significant impact on the internal market due to its reach._ID.RA-05_GV.RM-06_GV.OC-01_High_definition
2_The AI Office should engage with the scientific community, industry, civil society and other experts. Thresholds, as well as tools and benchmarks for the assessment of high-impact capabilities, should be strong predictors of generality, its capabilities and associated systemic risk of general-purpose AI models, and could take into account the way the model will be placed on the market or the number of users it may affect._GV.OC-02_ID.RA-05_ID.RA-06_High
3_The provider should notify the AI Office at the latest two weeks after the requirements are met or it becomes known that a general-purpose AI model will meet the requirements that lead to the presumption._RS.CO-02_GV.RR-02_GV.OC-03_High
4_Providers of general-purpose AI models presenting systemic risks should be subject, in addition to the obligations provided for providers of general-purpose AI models, to obligations aimed at identifying and mitigating those risks and ensuring an adequate level of cybersecurity protection, regardless of whether it is provided as a standalone model or embedded in an AI system or a product._ID.RA-05; ID.RA-06_GV.SC-01_PR.IR-01_High
4_This Regulation should require providers to perform the necessary model evaluations, in particular prior to its first placing on the market, including conducting and documenting adversarial testing of models, also, as appropriate, through internal or independent external testing._ID.IM-02_ID.RA-09_DE.AE-02_High
4_Providers of general-purpose AI models with systemic risks should continuously assess and mitigate systemic risks, including for example by putting in place risk-management policies, such as accountability and governance processes, implementing post-market monitoring, taking appropriate measures along the entire model's lifecycle and cooperating with relevant actors along the AI value chain._GV.RM-01; GV.RM-03_ID.RA-05_ID.AM-08_High
5_Providers of general-purpose AI models with systemic risks should assess and mitigate possible systemic risks. If, despite efforts to identify and prevent risks related to a general-purpose AI model that may present systemic risks, the development or use of the model causes a serious incident, the general-purpose AI model provider should without undue delay keep track of the incident and report any relevant information and possible corrective measures to the Commission and national competent authorities._RS.CO-02; RS.CO-03_ID.RA-06_DE.AE-04_High
5_Providers should ensure an adequate level of cybersecurity protection for the model and its physical infrastructure, if appropriate, along the entire model lifecycle. Cybersecurity protection related to systemic risks associated with malicious use or attacks should duly consider accidental model leakage, unauthorised releases, circumvention of safety measures, and defence against cyberattacks, unauthorised access or model theft._PR.DS-01; PR.DS-02_PR.AA-01_PR.IR-01_High
5_That protection could be facilitated by securing model weights, algorithms, servers, and data sets, such as through operational security measures for information security, specific cybersecurity policies, adequate technical and established solutions, and cyber and physical access controls, appropriate to the relevant circumstances and the risks involved._PR.DS-01_PR.AA-05_PR.AA-06_Medium
12_High-risk AI systems should be subject to a conformity assessment prior to their placing on the market or putting into service._ID.RA-09_GV.RM-06_ID.RA-05_High
13_Given the complexity of high-risk AI systems and the risks that are associated with them, it is important to develop an adequate conformity assessment procedure for high-risk AI systems involving notified bodies, so-called third party conformity assessment._ID.RA-09_GV.SC-01_ID.RA-05_High
15_Whenever a change occurs which may affect the compliance of a high-risk AI system with this Regulation (e.g. change of operating system or software architecture), or when the intended purpose of the system changes, that AI system should be considered to be a new AI system which should undergo a new conformity assessment._ID.RA-07_PR.PS-01_ID.RA-09_High
17_This section of the EU database should be publicly accessible, free of charge, the information should be easily navigable, understandable and machine-readable._RS.CO-03_PR.DS-01_GV.OC-04_Medium
18_Natural persons should be notified that they are interacting with an AI system, unless this is obvious from the point of view of a natural person who is reasonably well-informed, observant and circumspect taking into account the circumstances and the context of use._RS.CO-02_GV.OC-02_PR.AT-01_High_transparency
19_It is appropriate to require providers of those systems to embed technical solutions that enable marking in a machine readable format and detection that the output has been generated or manipulated by an AI system and not a human._PR.DS-01_ID.AM-08_PR.PS-04_High_transparency
19_Such techniques and methods should be sufficiently reliable, interoperable, effective and robust as far as this is technically feasible, taking into account available techniques or a combination of such techniques, such as watermarks, metadata identifications, cryptographic methods for proving provenance and authenticity of content, logging methods, fingerprints or other techniques._PR.DS-01; PR.DS-02_PR.PS-04_ID.AM-08_Medium_transparency
20_Deployers who use an AI system to generate or manipulate image, audio or video content that appreciably resembles existing persons, objects, places, entities or events and would falsely appear to a person to be authentic or truthful (deep fakes), should also clearly and distinguishably disclose that the content has been artificially created or manipulated by labelling the AI output accordingly and disclosing its artificial origin._RS.CO-02_PR.AT-01_GV.OC-02_High_transparency
24_The supervision of the AI systems in the AI regulatory sandbox should therefore cover their development, training, testing and validation before the systems are placed on the market or put into service, as well as the notion and occurrence of substantial modification that may require a new conformity assessment procedure._ID.IM-02_ID.RA-09_PR.PS-06_High
24_Any significant risks identified during the development and testing of such AI systems should result in adequate mitigation and, failing that, in the suspension of the development and testing process._ID.RA-06_RS.MI-01; RS.MI-02_ID.RA-05_High
25_The AI Office should cooperate with relevant national competent authorities, including those supervising the protection of fundamental rights._GV.OC-02_RS.CO-03_GV.RR-02_Medium
26_Providers and prospective providers in the AI regulatory sandbox should ensure appropriate safeguards and cooperate with the competent authorities, including by following their guidance and acting expeditiously and in good faith to adequately mitigate any identified significant risks to safety, health, and fundamental rights that may arise during the development, testing and experimentation in that sandbox._GV.RR-02_ID.RA-06_RS.MI-01_High
37_The national competent authorities should exercise their powers independently, impartially and without bias, so as to safeguard the principles of objectivity of their activities and tasks and to ensure the application and implementation of this Regulation._GV.RR-01_GV.OC-03_GV.PO-01_Medium
37_The members of these authorities should refrain from any action incompatible with their duties and should be subject to confidentiality rules under this Regulation._GV.RR-02_PR.AT-02_PR.DS-01_Medium
38_AI is a rapidly developing family of technologies that requires regulatory oversight and a safe and controlled space for experimentation, while ensuring responsible innovation and integration of appropriate safeguards and risk mitigation measures._GV.RM-01_ID.RA-05_GV.SC-01_Medium_definition
40_It is also important to minimise the risks and enable oversight by competent authorities and therefore require prospective providers to have a real-world testing plan submitted to competent market surveillance authority, register the testing in dedicated sections in the EU database subject to some limited exceptions._ID.IM-02_GV.OV-03_ID.RA-09_High
49_The development of AI systems other than high-risk AI systems in accordance with the requirements of this Regulation may lead to a larger uptake of ethical and trustworthy AI in the Union._GV.RR-01_GV.OC-01_ID.RA-05_Low_definition
50_All parties involved in the application of this Regulation should respect the confidentiality of information and data obtained in carrying out their tasks, in accordance with Union or national law._PR.DS-01_GV.OC-03_PR.AT-02_High
50_They should carry out their tasks and activities in such a manner as to protect, in particular, intellectual property rights, confidential business information and trade secrets, the effective implementation of this Regulation, public and national security interests, the integrity of criminal and administrative proceedings, and the integrity of classified information._PR.DS-01; PR.DS-02_GV.OC-03_PR.AT-02_High