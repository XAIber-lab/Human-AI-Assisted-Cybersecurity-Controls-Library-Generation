3_Sub-Measures and KPIs should be specific. We accept that Measures may be written at a higher level of generality than Sub-Measures and the KPIs evidencing compliance with Sub-Measures. Nonetheless, general-purpose AI model providers should have as clear of an understanding as possible of how to satisfy Sub-Measures, as appropriate, evidenced by KPIs._GV.PO-01; GV.PO-02_GV.RR-02_GV.RM-01__High
3_Measures, Sub-Measures and KPIs should be more stringent for more significant risks or uncertain risks of severe harm. The Code can accomplish this by, for example, suggesting a multitude of KPIs for each Sub-Measure related to a severe risk, thereby requiring providers of general-purpose AI models to take action to mitigate that severe risk or to robustly demonstrate an extremely rare likelihood of severe risk eventuating._ID.RA-05; ID.RA-06_GV.RM-02_GV.RM-06__High
3_Measures, Sub-Measures and KPIs should differentiate, where applicable, between different types of risks, distribution strategies, deployment contexts, and other factors that may influence the level of risk, and how risks may need to be assessed and mitigated._ID.RA-05_GV.RM-06_ID.RA-04__High
3_The Code might also tie risk-mitigating Sub-Measures to risk-assessment KPIs, such as using "if-then" requirements. For example, if a general-purpose AI model with systemic risk is assessed to have capability X, Y risk mitigations must be in place, guided by Z KPIs._ID.RA-06_GV.RM-04_ID.RA-05__High
4_The Code can accomplish this by, for example, referencing dynamic sources of information that providers can be expected to monitor and consider themselves. Examples of such sources could include incident databases, consensus standards, risk registers, risk management frameworks, and AI Office guidance._ID.RA-02_DE.AE-07_RS.CO-03__Medium
4_Measures and KPIs related to the obligations applicable to providers of general-purpose AI models should take due account of the size of the general-purpose AI model provider and allow simplified ways of compliance for SMEs and start-ups with fewer financial resources than those at the frontier of AI development, where appropriate._GV.RR-03_GV.PO-01_GV.OC-02__High
4_We encourage further transparency between stakeholders and increased efforts to share knowledge and cooperate in building a collective and robust evidence base for AI Safety._RS.CO-03_ID.RA-02_DE.AE-07__Medium
4_Measures, Sub-Measures and KPIs should differentiate between intentional and unintentional (including misalignment) risks, and might be more or less specific and stringent for some types of risks, distribution strategies (e.g. open-sourcing), and deployment contexts than for others._ID.RA-03; ID.RA-04_ID.RA-05_GV.RM-06__High
4_Sub-Measures and KPIs should preserve the AI Office's ability to improve its assessment of compliance based on superior information._GV.OV-03_ID.IM-01_GV.OV-01__Medium
4_The process for updating Sub-Measures and KPIs should assume that rapid technological change may require agile regulatory development and modification._GV.PO-02_ID.IM-01_GV.OV-02__Medium
6_The Code serves as a guiding document for providers of GPAI models and general-purpose AI models with systemic risk in demonstrating compliance with the AI Act, while recognising that adherence to this Code does not constitute conclusive evidence of compliance with the AI Act._GV.PO-01_GV.OC-03___Medium (definition)
6_The absence of specific Measures, Sub-Measures, and Key Performance Indicators (KPIs) within this Code does not absolve providers of general-purpose AI models with systemic risk of their responsibility to address and mitigate potential systemic risks as they emerge._GV.RR-01_ID.RA-05_GV.RM-03__High
8_The Signatories recognise the particular role and responsibility of providers of general-purpose AI models along the AI value chain, as the models they provide may form the basis for a range of downstream systems._GV.RR-01_GV.SC-02___Low (definition)
10-12_Detailed documentation requirements for AI Office and downstream providers including: model name, evidence of provenance and authenticity, intended tasks and types of AI systems for integration, acceptable use policies, date of release and distribution methods, interaction with external hardware/software, architecture details, input/output modalities, licensing information, technical integration means, design specifications, training process details, data usage information, computational resources used, and energy consumption metrics._ID.AM-02; ID.AM-08_ID.AM-07_PR.PS-04__High
13_The AUP should contain: purpose statement, scope definition, primary intended uses and users, acceptable/unacceptable uses, security measures, monitoring and privacy policies, warning processes, suspension criteria, and user acknowledgment requirements._GV.PO-01_PR.AA-05_PR.DS-01_privacy_High
13_The AUP should contain details about the security protocols that the users of the general-purpose AI systems must follow._PR.AA-05_PR.DS-01_PR.PS-01__High
13_Warning processes and criteria for suspension or withdrawal of user privileges for not adhering to the AUP._PR.AA-05_GV.PO-01_RS.MI-01__High
13_Criteria for terminating user accounts and reference to applicable law and regulations for enforcement._PR.AA-05_GV.OC-03_GV.PO-01__High
14_Signatories will draw up and implement an internal policy to comply with Union law on copyright and related rights in line with this Chapter of the Code. This policy shall cover the entire lifecycle of any general-purpose AI model subject to this Code._GV.PO-01; GV.PO-02_GV.OC-03_ID.AM-08__High
14-15_Signatories will undertake reasonable copyright due diligence before entering into contracts with third parties about data usage. Implementation of downstream copyright measures to mitigate risks of copyright infringement._GV.SC-06_ID.RA-09_GV.SC-05__High
15_When engaging in text and data mining, signatories must ensure lawful access to copyright-protected content and comply with rights reservations. This includes respecting robots.txt protocols and avoiding negative impacts on content findability._PR.AA-05_GV.OC-03_PR.DS-01__High
16_Signatories commit to adequate transparency about copyright compliance measures, including making public information about rights reservation compliance and providing a single point of contact for rightsholders._RS.CO-03_GV.PO-01_GV.SC-02_transparency_High
17_Issues related to the inability to control powerful autonomous general-purpose AI models._ID.RA-03_ID.RA-04___Low (definition)
17_The Signatories recognise that the taxonomy of systemic risks is non-exhaustive and will be subject to change over time, reflecting scientific advances and societal changes._GV.RM-02_ID.RA-03___Medium (definition)
17_The taxonomy has been developed and, when in doubt, should be interpreted in light of the severity and probability of each risk as defined in Article 3(2) of the AI Act._ID.RA-05_GV.RM-06_GV.OC-03__High
17-18_Taxonomy of systemic risks including: cyber offence, CBRN risks, loss of control, automated AI R&D risks, manipulation risks, and large-scale discrimination. Consideration of accidents, privacy infringements, and impacts on public health, safety, democracy, infrastructure, rights, environment, and economy._ID.RA-03; ID.RA-04_ID.RA-05_GV.RM-06__Medium (definition)
18_Sources of risks, also referred to as "factors of risks" or "drivers of risks," are elements (e.g. events, components, actors and their intentions or activities) that alone or in combination give rise to risks._ID.RA-03_ID.RA-04___Low (definition)
18_Velocity at which the risk materialises: Gradual, sudden, continuously changing._ID.RA-04_DE.AE-04___Low (definition)
18_Visibility of the risk while it materialises: Overt (open), covert (hidden)._ID.RA-04_DE.CM-09___Low (definition)
18_Course of events: Linear, recursive (feedback loops), compound, cascading (chain reactions)._ID.RA-04_DE.AE-02___Low (definition)